{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Today we will learn about two kinds of prediction!\n",
    "The first one is __cold/hot__ method.\n",
    "The most important part of this method is triple error comparison with fixed weight change. This is the main disadvantage of this method, it is not flexible."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For weight 1.5 prediction is 0.25, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n",
      "For weight 1.5 prediction is 0.75, error is 0.0025000000000000044\n"
     ]
    }
   ],
   "source": [
    "weight = 0.5\n",
    "input = 0.5\n",
    "goal_prediction = 0.8\n",
    "delta = 1\n",
    "\n",
    "for i in range(1001):\n",
    "    prediction = weight * input\n",
    "    q_error = (prediction - goal_prediction) ** 2\n",
    "    up_weight = weight + delta\n",
    "    up_prediction = up_weight * input\n",
    "    q_up_error = (up_prediction - goal_prediction) ** 2\n",
    "    down_weight = weight - delta\n",
    "    down_prediction = down_weight * input\n",
    "    q_down_error = (down_prediction - goal_prediction) ** 2\n",
    "    if q_error > q_down_error and q_error < q_up_error:\n",
    "        q_error = q_down_error\n",
    "        weight = down_weight\n",
    "    elif q_error < q_down_error and q_up_error < q_error:\n",
    "        q_error = q_up_error\n",
    "        weight = up_weight\n",
    "    print(\"For weight {0} prediction is {2}, error is {1}\". format(weight, q_error, prediction))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "But this method is not perfect because it is not possible to find an ideal dot. Moreover, it calculates too much for\n",
    " one\n",
    "iteration.\n",
    "Let's learn the method is called **gradient descend**\n",
    "The main features are reversal of a sign, stop, scaling.\n",
    "And also **alpha** coefficient that means a speef of learning (sometimes this coefficient is quite useful when you\n",
    "use a big input meaning and small error. This even is called **Discrepancy/overcorrection of weight**"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The error is 0.765625 for weight 0.271875 and prediction 0.125.\n",
      "The error is 0.7466040039062499 for weight 0.2934765625 and prediction 0.1359375.\n",
      "The error is 0.7280555606842041 for weight 0.31480810546875 and prediction 0.14673828125.\n",
      "The error is 0.709967930348456 for weight 0.3358730041503906 and prediction 0.157404052734375.\n",
      "The error is 0.6923296645788615 for weight 0.3566745915985107 and prediction 0.1679365020751953.\n",
      "The error is 0.6751295994744804 for weight 0.37721615920352936 and prediction 0.17833729579925536.\n",
      "The error is 0.6583568484875363 for weight 0.3975009572134852 and prediction 0.18860807960176468.\n",
      "The error is 0.642000795532924 for weight 0.41753219524831664 and prediction 0.1987504786067426.\n",
      "The error is 0.626051088268903 for weight 0.43731304280771266 and prediction 0.20876609762415832.\n",
      "The error is 0.6104976315447225 for weight 0.45684662977261625 and prediction 0.21865652140385633.\n",
      "The error is 0.5953305810110331 for weight 0.4761360469004585 and prediction 0.22842331488630813.\n",
      "The error is 0.5805403368890404 for weight 0.4951843463142028 and prediction 0.23806802345022926.\n",
      "The error is 0.5661175378944533 for weight 0.5139945419852753 and prediction 0.2475921731571014.\n",
      "The error is 0.552053055312388 for weight 0.5325696102104593 and prediction 0.25699727099263764.\n",
      "The error is 0.5383379872194708 for weight 0.5509124900828286 and prediction 0.26628480510522967.\n",
      "The error is 0.5249636528494871 for weight 0.5690260839567932 and prediction 0.2754562450414143.\n",
      "The error is 0.5119215870990077 for weight 0.5869132579073333 and prediction 0.2845130419783966.\n",
      "The error is 0.4992035351695167 for weight 0.6045768421834916 and prediction 0.29345662895366664.\n",
      "The error is 0.486801447342649 for weight 0.622019631656198 and prediction 0.3022884210917458.\n",
      "The error is 0.47470747388523016 for weight 0.6392443862604955 and prediction 0.311009815828099.\n",
      "The error is 0.46291396008089386 for weight 0.6562538314322394 and prediction 0.31962219313024776.\n",
      "The error is 0.45141344138513406 for weight 0.6730506585393363 and prediction 0.3281269157161197.\n",
      "The error is 0.44019863870072223 for weight 0.6896375253075946 and prediction 0.33652532926966816.\n",
      "The error is 0.4292624537705012 for weight 0.7060170562412497 and prediction 0.3448187626537973.\n",
      "The error is 0.4185979646846404 for weight 0.722191843038234 and prediction 0.35300852812062483.\n",
      "The error is 0.40819842149950636 for weight 0.7381644450002561 and prediction 0.361095921519117.\n",
      "The error is 0.398057241965378 for weight 0.7539373894377529 and prediction 0.36908222250012807.\n",
      "The error is 0.38816800736030066 for weight 0.769513172069781 and prediction 0.37696869471887645.\n",
      "The error is 0.3785244584274431 for weight 0.7848942574189087 and prediction 0.3847565860348905.\n",
      "The error is 0.36912049141338626 for weight 0.8000830792011724 and prediction 0.39244712870945436.\n",
      "The error is 0.359950154204835 for weight 0.8150820407111578 and prediction 0.4000415396005862.\n",
      "The error is 0.35100764256130856 for weight 0.8298935152022683 and prediction 0.4075410203555789.\n",
      "The error is 0.34228729644142614 for weight 0.84451984626224 and prediction 0.41494675760113414.\n",
      "The error is 0.33378359642045946 for weight 0.858963348183962 and prediction 0.42225992313112.\n",
      "The error is 0.32549116019688856 for weight 0.8732263063316625 and prediction 0.429481674091981.\n",
      "The error is 0.3174047391857471 for weight 0.8873109775025168 and prediction 0.43661315316583127.\n",
      "The error is 0.3095192151966012 for weight 0.9012195902837353 and prediction 0.4436554887512584.\n",
      "The error is 0.30182959719406055 for weight 0.9149543454051886 and prediction 0.45060979514186766.\n",
      "The error is 0.2943310181387706 for weight 0.9285174160876237 and prediction 0.4574771727025943.\n",
      "The error is 0.2870187319068856 for weight 0.9419109483865284 and prediction 0.46425870804381186.\n",
      "The error is 0.2798881102860739 for weight 0.9551370615316969 and prediction 0.4709554741932642.\n",
      "The error is 0.2729346400461543 for weight 0.9681978482625506 and prediction 0.47756853076584843.\n",
      "The error is 0.26615392008250766 for weight 0.9810953751592687 and prediction 0.4840989241312753.\n",
      "The error is 0.2595416586304578 for weight 0.9938316829697779 and prediction 0.49054768757963435.\n",
      "The error is 0.25309367054885745 for weight 1.0064087869326557 and prediction 0.49691584148488893.\n",
      "The error is 0.24680587467115922 for weight 1.0188286770959976 and prediction 0.5032043934663278.\n",
      "The error is 0.24067429122229755 for weight 1.0310933186322977 and prediction 0.5094143385479988.\n",
      "The error is 0.23469503929974356 for weight 1.043204652149394 and prediction 0.5155466593161488.\n",
      "The error is 0.2288643344171405 for weight 1.0551645939975267 and prediction 0.521602326074697.\n",
      "The error is 0.22317848610896462 for weight 1.0669750365725577 and prediction 0.5275822969987634.\n",
      "The error is 0.21763389559469504 for weight 1.0786378486154007 and prediction 0.5334875182862788.\n",
      "The error is 0.21222705350101434 for weight 1.0901548755077082 and prediction 0.5393189243077003.\n",
      "The error is 0.2069545376405985 for weight 1.1015279395638617 and prediction 0.5450774377538541.\n",
      "The error is 0.2018130108460899 for weight 1.1127588403193134 and prediction 0.5507639697819309.\n",
      "The error is 0.1967992188578824 for weight 1.1238493548153219 and prediction 0.5563794201596567.\n",
      "The error is 0.19190998826438194 for weight 1.1348012378801304 and prediction 0.5619246774076609.\n",
      "The error is 0.18714222449343867 for weight 1.1456162224066289 and prediction 0.5674006189400652.\n",
      "The error is 0.18249290985367977 for weight 1.156296019626546 and prediction 0.5728081112033144.\n",
      "The error is 0.17795910162450243 for weight 1.166842319381214 and prediction 0.578148009813273.\n",
      "The error is 0.17353793019351874 for weight 1.1772567903889488 and prediction 0.583421159690607.\n",
      "The error is 0.16922659724027353 for weight 1.187541080509087 and prediction 0.5886283951944744.\n",
      "The error is 0.1650223739650855 for weight 1.1976968170027233 and prediction 0.5937705402545435.\n",
      "The error is 0.1609225993618904 for weight 1.2077256067901894 and prediction 0.5988484085013617.\n",
      "The error is 0.15692467853399342 for weight 1.217629036705312 and prediction 0.6038628033950947.\n",
      "The error is 0.15302608105166451 for weight 1.2274086737464955 and prediction 0.608814518352656.\n",
      "The error is 0.14922433935053725 for weight 1.2370660653246643 and prediction 0.6137043368732478.\n",
      "The error is 0.14551704716979733 for weight 1.246602739508106 and prediction 0.6185330326623322.\n",
      "The error is 0.14190185802917266 for weight 1.2560202052642548 and prediction 0.623301369754053.\n",
      "The error is 0.13837648374376038 for weight 1.2653199526984515 and prediction 0.6280101026321274.\n",
      "The error is 0.13493869297575137 for weight 1.2745034532897208 and prediction 0.6326599763492258.\n",
      "The error is 0.13158630982213507 for weight 1.2835721601235992 and prediction 0.6372517266448604.\n",
      "The error is 0.12831721243749145 for weight 1.2925275081220542 and prediction 0.6417860800617996.\n",
      "The error is 0.12512933169099752 for weight 1.3013709142705285 and prediction 0.6462637540610271.\n",
      "The error is 0.1220206498567993 for weight 1.310103777842147 and prediction 0.6506854571352643.\n",
      "The error is 0.11898919933691944 for weight 1.31872748061912 and prediction 0.6550518889210735.\n",
      "The error is 0.11603306141589287 for weight 1.3272433871113811 and prediction 0.65936374030956.\n",
      "The error is 0.11315036504634175 for weight 1.335652844772489 and prediction 0.6636216935556906.\n",
      "The error is 0.11033928566472168 for weight 1.3439571842128328 and prediction 0.6678264223862445.\n",
      "The error is 0.10759804403648873 for weight 1.3521577194101724 and prediction 0.6719785921064164.\n",
      "The error is 0.10492490512995724 for weight 1.3602557479175452 and prediction 0.6760788597050862.\n",
      "The error is 0.10231817701813486 for weight 1.368252551068576 and prediction 0.6801278739587726.\n",
      "The error is 0.09977620980784058 for weight 1.3761493941802188 and prediction 0.684126275534288.\n",
      "The error is 0.09729739459542702 for weight 1.383947526752966 and prediction 0.6880746970901094.\n",
      "The error is 0.0948801624484469 for weight 1.3916481826685538 and prediction 0.691973763376483.\n",
      "The error is 0.09252298341261832 for weight 1.399252580385197 and prediction 0.6958240913342769.\n",
      "The error is 0.09022436554346107 for weight 1.406761923130382 and prediction 0.6996262901925985.\n",
      "The error is 0.08798285396199068 for weight 1.4141773990912523 and prediction 0.703380961565191.\n",
      "The error is 0.08579702993387248 for weight 1.4215001816026116 and prediction 0.7070886995456261.\n",
      "The error is 0.08366550997145283 for weight 1.428731429332579 and prediction 0.7107500908013058.\n",
      "The error is 0.08158694495809954 for weight 1.4358722864659217 and prediction 0.7143657146662895.\n",
      "The error is 0.07956001929429678 for weight 1.4429238828850977 and prediction 0.7179361432329608.\n",
      "The error is 0.0775834500649541 for weight 1.449887334349034 and prediction 0.7214619414425488.\n",
      "The error is 0.0756559862274029 for weight 1.4567637426696711 and prediction 0.724943667174517.\n",
      "The error is 0.07377640781956582 for weight 1.4635541958863003 and prediction 0.7283818713348356.\n",
      "The error is 0.07194352518779847 for weight 1.4702597684377217 and prediction 0.7317770979431502.\n",
      "The error is 0.07015617823391407 for weight 1.4768815213322501 and prediction 0.7351298842188608.\n",
      "The error is 0.06841323568091527 for weight 1.483420502315597 and prediction 0.7384407606661251.\n",
      "The error is 0.06671359435696751 for weight 1.4898777460366521 and prediction 0.7417102511577985.\n",
      "The error is 0.0650561784971616 for weight 1.496254274211194 and prediction 0.7449388730183261.\n",
      "The error is 0.06343993906262274 for weight 1.502551095783554 and prediction 0.748127137105597.\n",
      "The error is 0.06186385307653573 for weight 1.5087692070862595 and prediction 0.751275547891777.\n",
      "The error is 0.06032692297666555 for weight 1.5149095919976812 and prediction 0.7543846035431298.\n",
      "The error is 0.05882817598396404 for weight 1.5209732220977101 and prediction 0.7574547959988406.\n",
      "The error is 0.05736666348686244 for weight 1.5269610568214889 and prediction 0.7604866110488551.\n",
      "The error is 0.05594146044086067 for weight 1.5328740436112203 and prediction 0.7634805284107444.\n",
      "The error is 0.05455166478303304 for weight 1.53871311806608 and prediction 0.7664370218056101.\n",
      "The error is 0.053196396861079555 for weight 1.544479204090254 and prediction 0.76935655903304.\n",
      "The error is 0.05187479887656213 for weight 1.5501732140391258 and prediction 0.772239602045127.\n",
      "The error is 0.050586034341972536 for weight 1.5557960488636366 and prediction 0.7750866070195629.\n",
      "The error is 0.04932928755128917 for weight 1.5613485982528412 and prediction 0.7778980244318183.\n",
      "The error is 0.04810376306368682 for weight 1.5668317407746808 and prediction 0.7806742991264206.\n",
      "The error is 0.04690868520007333 for weight 1.5722463440149972 and prediction 0.7834158703873404.\n",
      "The error is 0.045743297552134035 for weight 1.5775932647148097 and prediction 0.7861231720074986.\n",
      "The error is 0.044606862503573215 for weight 1.5828733489058746 and prediction 0.7887966323574048.\n",
      "The error is 0.043498660763250055 for weight 1.5880874320445513 and prediction 0.7914366744529373.\n",
      "The error is 0.04241799090991304 for weight 1.5932363391439943 and prediction 0.7940437160222756.\n",
      "The error is 0.0413641689482449 for weight 1.5983208849046944 and prediction 0.7966181695719972.\n",
      "The error is 0.040336527875936945 for weight 1.6033418738433858 and prediction 0.7991604424523472.\n",
      "The error is 0.03933441726151912 for weight 1.6083001004203434 and prediction 0.8016709369216929.\n",
      "The error is 0.03835720283267826 for weight 1.6131963491650891 and prediction 0.8041500502101717.\n",
      "The error is 0.03740426607480391 for weight 1.6180313948005256 and prediction 0.8065981745825446.\n",
      "The error is 0.03647500383950798 for weight 1.622806002365519 and prediction 0.8090156974002628.\n",
      "The error is 0.0355688279628702 for weight 1.62752092733595 and prediction 0.8114030011827595.\n",
      "The error is 0.03468516489316764 for weight 1.6321769157442507 and prediction 0.813760463667975.\n",
      "The error is 0.03382345532785301 for weight 1.6367747042974476 and prediction 0.8160884578721254.\n",
      "The error is 0.032983153859551646 for weight 1.6413150204937295 and prediction 0.8183873521487238.\n",
      "The error is 0.03216372863085343 for weight 1.6457985827375579 and prediction 0.8206575102468647.\n",
      "The error is 0.03136466099768066 for weight 1.6502261004533383 and prediction 0.8228992913687789.\n",
      "The error is 0.03058544520101955 for weight 1.6545982741976715 and prediction 0.8251130502266691.\n",
      "The error is 0.029825588046806736 for weight 1.6589157957702005 and prediction 0.8272991370988357.\n",
      "The error is 0.029084608593768886 for weight 1.663179348323073 and prediction 0.8294578978851003.\n",
      "The error is 0.028362037849017443 for weight 1.6673896064690346 and prediction 0.8315896741615365.\n",
      "The error is 0.027657418471205925 for weight 1.6715472363881716 and prediction 0.8336948032345173.\n",
      "The error is 0.026970304481061906 for weight 1.6756528959333195 and prediction 0.8357736181940858.\n",
      "The error is 0.026300260979110524 for weight 1.679707234734153 and prediction 0.8378264479666597.\n",
      "The error is 0.025646863870410732 for weight 1.6837108942999761 and prediction 0.8398536173670765.\n",
      "The error is 0.02500969959613022 for weight 1.6876645081212265 and prediction 0.8418554471499881.\n",
      "The error is 0.02438836487178885 for weight 1.6915687017697112 and prediction 0.8438322540606132.\n",
      "The error is 0.02378246643200534 for weight 1.69542409299759 and prediction 0.8457843508848556.\n",
      "The error is 0.023191620781585192 for weight 1.69923129183512 and prediction 0.847712046498795.\n",
      "The error is 0.02261545395279269 for weight 1.702990900687181 and prediction 0.84961564591756.\n",
      "The error is 0.022053601268652984 for weight 1.7067035144285914 and prediction 0.8514954503435905.\n",
      "The error is 0.02150570711213488 for weight 1.710369720498234 and prediction 0.8533517572142957.\n",
      "The error is 0.020971424701067776 for weight 1.713990098992006 and prediction 0.855184860249117.\n",
      "The error is 0.020450415868650623 for weight 1.7175652227546059 and prediction 0.856995049496003.\n",
      "The error is 0.01994235084941385 for weight 1.7210956574701732 and prediction 0.8587826113773029.\n",
      "The error is 0.019446908070498734 for weight 1.7245819617517961 and prediction 0.8605478287350866.\n",
      "The error is 0.018963773948122275 for weight 1.7280246872298988 and prediction 0.8622909808758981.\n",
      "The error is 0.018492642689098598 for weight 1.7314243786395251 and prediction 0.8640123436149494.\n",
      "The error is 0.01803321609729129 for weight 1.734781573906531 and prediction 0.8657121893197626.\n",
      "The error is 0.017585203384874206 for weight 1.7380968042326994 and prediction 0.8673907869532655.\n",
      "The error is 0.017148320988281243 for weight 1.7413705941797906 and prediction 0.8690484021163497.\n",
      "The error is 0.016722292388728646 for weight 1.7446034617525432 and prediction 0.8706852970898953.\n",
      "The error is 0.016306847937196164 for weight 1.7477959184806364 and prediction 0.8723017308762716.\n",
      "The error is 0.015901724683756446 for weight 1.7509484694996285 and prediction 0.8738979592403182.\n",
      "The error is 0.015506666211144375 for weight 1.754061613630883 and prediction 0.8754742347498142.\n",
      "The error is 0.015121422472461258 for weight 1.7571358434604971 and prediction 0.8770308068154415.\n",
      "The error is 0.01474574963291104 for weight 1.760171645417241 and prediction 0.8785679217302486.\n",
      "The error is 0.014379409915468405 for weight 1.7631694998495253 and prediction 0.8800858227086205.\n",
      "The error is 0.014022171450380998 for weight 1.7661298811014063 and prediction 0.8815847499247627.\n",
      "The error is 0.013673808128410583 for weight 1.7690532575876388 and prediction 0.8830649405507032.\n",
      "The error is 0.013334099457720383 for weight 1.7719400918677932 and prediction 0.8845266287938194.\n",
      "The error is 0.013002830424317649 for weight 1.7747908407194457 and prediction 0.8859700459338966.\n",
      "The error is 0.012679791355963518 for weight 1.7776059552104526 and prediction 0.8873954203597229.\n",
      "The error is 0.012364777789463802 for weight 1.780385880770322 and prediction 0.8888029776052263.\n",
      "The error is 0.012057590341256818 for weight 1.7831310572606929 and prediction 0.890192940385161.\n",
      "The error is 0.011758034581216218 for weight 1.7858419190449342 and prediction 0.8915655286303464.\n",
      "The error is 0.011465920909589132 for weight 1.7885188950568724 and prediction 0.8929209595224671.\n",
      "The error is 0.011181064436991532 for weight 1.7911624088686615 and prediction 0.8942594475284362.\n",
      "The error is 0.010903284867385026 for weight 1.7937728787578033 and prediction 0.8955812044343308.\n",
      "The error is 0.01063240638396092 for weight 1.7963507177733309 and prediction 0.8968864393789017.\n",
      "The error is 0.010368257537859383 for weight 1.7988963338011643 and prediction 0.8981753588866654.\n",
      "The error is 0.010110671139653181 for weight 1.8014101296286498 and prediction 0.8994481669005822.\n",
      "The error is 0.009859484153527422 for weight 1.8038925030082917 and prediction 0.9007050648143249.\n",
      "The error is 0.009614537594088217 for weight 1.806343846720688 and prediction 0.9019462515041459.\n",
      "The error is 0.009375676425735092 for weight 1.8087645486366795 and prediction 0.903171923360344.\n",
      "The error is 0.009142749464533233 for weight 1.8111549917787209 and prediction 0.9043822743183397.\n",
      "The error is 0.008915609282523747 for weight 1.813515554381487 and prediction 0.9055774958893604.\n",
      "The error is 0.00869411211441104 for weight 1.8158466099517183 and prediction 0.9067577771907435.\n",
      "The error is 0.008478117766568642 for weight 1.818148527327322 and prediction 0.9079233049758592.\n",
      "The error is 0.008267489528305447 for weight 1.8204216707357304 and prediction 0.909074263663661.\n",
      "The error is 0.00806209408533661 for weight 1.8226663998515338 and prediction 0.9102108353678652.\n",
      "The error is 0.007861801435404025 for weight 1.8248830698533896 and prediction 0.9113331999257669.\n",
      "The error is 0.007666484805993211 for weight 1.8270720314802222 and prediction 0.9124415349266948.\n",
      "The error is 0.007476020574094312 for weight 1.8292336310867194 and prediction 0.9135360157401111.\n",
      "The error is 0.007290288187956659 for weight 1.8313682106981355 and prediction 0.9146168155433597.\n",
      "The error is 0.007109170090787107 for weight 1.8334761080644089 and prediction 0.9156841053490677.\n",
      "The error is 0.006932551646344109 for weight 1.8355576567136038 and prediction 0.9167380540322044.\n",
      "The error is 0.006760321066380241 for weight 1.8376131860046838 and prediction 0.9177788283568019.\n",
      "The error is 0.006592369339887354 for weight 1.8396430211796253 and prediction 0.9188065930023419.\n",
      "The error is 0.006428590164099524 for weight 1.84164748341488 and prediction 0.9198215105898127.\n",
      "The error is 0.006268879877210181 for weight 1.843626889872194 and prediction 0.92082374170744.\n",
      "The error is 0.006113137392760743 for weight 1.8455815537487914 and prediction 0.921813444936097.\n",
      "The error is 0.005961264135659349 for weight 1.8475117843269315 and prediction 0.9227907768743957.\n",
      "The error is 0.005813163979789061 for weight 1.8494178870228448 and prediction 0.9237558921634658.\n",
      "The error is 0.005668743187166181 for weight 1.8513001634350592 and prediction 0.9247089435114224.\n",
      "The error is 0.005527910348610026 for weight 1.853158911392121 and prediction 0.9256500817175296.\n",
      "The error is 0.005390576325886746 for weight 1.8549944249997194 and prediction 0.9265794556960605.\n",
      "The error is 0.005256654195290497 for weight 1.856806994687223 and prediction 0.9274972124998597.\n",
      "The error is 0.005126059192626246 for weight 1.8585969072536328 and prediction 0.9284034973436115.\n",
      "The error is 0.004998708659559431 for weight 1.8603644459129625 and prediction 0.9292984536268164.\n",
      "The error is 0.004874521991298496 for weight 1.8621098903390505 and prediction 0.9301822229564812.\n",
      "The error is 0.00475342058557717 for weight 1.8638335167098123 and prediction 0.9310549451695252.\n",
      "The error is 0.004635327792904239 for weight 1.8655355977509398 and prediction 0.9319167583549062.\n",
      "The error is 0.004520168868049267 for weight 1.867216402779053 and prediction 0.9327677988754699.\n",
      "The error is 0.004407870922733672 for weight 1.868876197744315 and prediction 0.9336082013895265.\n",
      "The error is 0.004298362879497002 for weight 1.870515245272511 and prediction 0.9344380988721575.\n",
      "The error is 0.004191575426709503 for weight 1.8721338047066045 and prediction 0.9352576226362554.\n",
      "The error is 0.004087440974702189 for weight 1.873732132147772 and prediction 0.9360669023533023.\n",
      "The error is 0.003985893612986928 for weight 1.8753104804959249 and prediction 0.936866066073886.\n",
      "The error is 0.003886869068539284 for weight 1.8768690994897257 and prediction 0.9376552402479624.\n",
      "The error is 0.0037903046651177677 for weight 1.878408235746104 and prediction 0.9384345497448628.\n",
      "The error is 0.0036961392835937524 for weight 1.8799281327992778 and prediction 0.939204117873052.\n",
      "The error is 0.0036043133232669645 for weight 1.881429031139287 and prediction 0.9399640663996389.\n",
      "The error is 0.003514768664142046 for weight 1.882911168250046 and prediction 0.9407145155696435.\n",
      "The error is 0.003427448630142265 for weight 1.8843747786469203 and prediction 0.941455584125023.\n",
      "The error is 0.003342297953237171 for weight 1.8858200939138339 and prediction 0.9421873893234601.\n",
      "The error is 0.0032592627384614293 for weight 1.887247342739911 and prediction 0.9429100469569169.\n",
      "The error is 0.0031782904298027744 for weight 1.8886567509556622 and prediction 0.9436236713699555.\n",
      "The error is 0.003099329776937357 for weight 1.8900485415687165 and prediction 0.9443283754778311.\n",
      "The error is 0.0030223308027915665 for weight 1.8914229347991076 and prediction 0.9450242707843582.\n",
      "The error is 0.0029472447719097092 for weight 1.8927801481141189 and prediction 0.9457114673995538.\n",
      "The error is 0.0028740241596075713 for weight 1.8941203962626925 and prediction 0.9463900740570594.\n",
      "The error is 0.002802622621892317 for weight 1.8954438913094087 and prediction 0.9470601981313462.\n",
      "The error is 0.002732994966129685 for weight 1.896750842668041 and prediction 0.9477219456547044.\n",
      "The error is 0.002665097122439903 for weight 1.8980414571346906 and prediction 0.9483754213340205.\n",
      "The error is 0.002598886115804282 for weight 1.899315938920507 and prediction 0.9490207285673453.\n",
      "The error is 0.00253432003886477 for weight 1.9005744896840007 and prediction 0.9496579694602535.\n",
      "The error is 0.0024713580253992225 for weight 1.9018173085629506 and prediction 0.9502872448420003.\n",
      "The error is 0.0024099602244557128 for weight 1.9030445922059138 and prediction 0.9509086542814753.\n",
      "The error is 0.0023500877751293867 for weight 1.90425653480334 and prediction 0.9515222961029569.\n",
      "The error is 0.0022917027819660107 for weight 1.9054533281182982 and prediction 0.95212826740167.\n",
      "The error is 0.002234768290976548 for weight 1.9066351615168193 and prediction 0.9527266640591491.\n",
      "The error is 0.0021792482662476033 for weight 1.907802221997859 and prediction 0.9533175807584097.\n",
      "The error is 0.0021251075671330196 for weight 1.9089546942228857 and prediction 0.9539011109989295.\n",
      "The error is 0.0020723119260120625 for weight 1.9100927605450997 and prediction 0.9544773471114428.\n",
      "The error is 0.0020208279266001957 for weight 1.9112166010382858 and prediction 0.9550463802725498.\n",
      "The error is 0.001970622982798727 for weight 1.9123263935253072 and prediction 0.9556083005191429.\n",
      "The error is 0.0019216653180698239 for weight 1.913422313606241 and prediction 0.9561631967626536.\n",
      "The error is 0.0018739239453240235 for weight 1.9145045346861629 and prediction 0.9567111568031205.\n",
      "The error is 0.0018273686473073813 for weight 1.915573228002586 and prediction 0.9572522673430814.\n",
      "The error is 0.0017819699574758367 for weight 1.9166285626525537 and prediction 0.957786614001293.\n",
      "The error is 0.0017376991413447927 for weight 1.9176707056193967 and prediction 0.9583142813262768.\n",
      "The error is 0.0016945281783020106 for weight 1.9186998217991542 and prediction 0.9588353528096983.\n",
      "The error is 0.00165242974387232 for weight 1.9197160740266648 and prediction 0.9593499108995771.\n",
      "The error is 0.0016113771924229925 for weight 1.9207196231013315 and prediction 0.9598580370133324.\n",
      "The error is 0.001571344540298733 for weight 1.9217106278125649 and prediction 0.9603598115506657.\n",
      "The error is 0.001532306449375686 for weight 1.9226892449649078 and prediction 0.9608553139062824.\n",
      "The error is 0.0014942382110240092 for weight 1.9236556294028464 and prediction 0.9613446224824539.\n",
      "The error is 0.0014571157304688828 for weight 1.9246099340353109 and prediction 0.9618278147014232.\n",
      "The error is 0.0014209155115400439 for weight 1.9255523098598695 and prediction 0.9623049670176554.\n",
      "The error is 0.0013856146418002222 for weight 1.926482905986621 and prediction 0.9627761549299347.\n",
      "The error is 0.0013511907780429985 for weight 1.9274018696617883 and prediction 0.9632414529933105.\n",
      "The error is 0.0013176221321509942 for weight 1.928309346291016 and prediction 0.9637009348308941.\n",
      "The error is 0.0012848874573053685 for weight 1.9292054794623783 and prediction 0.964154673145508.\n",
      "The error is 0.0012529660345379351 for weight 1.9300904109690986 and prediction 0.9646027397311892.\n",
      "The error is 0.001221837659617382 for weight 1.930964280831985 and prediction 0.9650452054845493.\n",
      "The error is 0.0011914826302612618 for weight 1.931827227321585 and prediction 0.9654821404159925.\n",
      "The error is 0.0011618817336657092 for weight 1.9326793869800654 and prediction 0.9659136136607925.\n",
      "The error is 0.0011330162343449483 for weight 1.9335208946428146 and prediction 0.9663396934900327.\n",
      "The error is 0.0011048678622729382 for weight 1.9343518834597795 and prediction 0.9667604473214073.\n",
      "The error is 0.0010774188013195935 for weight 1.9351724849165322 and prediction 0.9671759417298897.\n",
      "The error is 0.0010506516779743129 for weight 1.9359828288550756 and prediction 0.9675862424582661.\n",
      "The error is 0.0010245495503496368 for weight 1.9367830434943871 and prediction 0.9679914144275378.\n",
      "The error is 0.0009990958974581375 for weight 1.9375732554507072 and prediction 0.9683915217471936.\n",
      "The error is 0.0009742746087556632 for weight 1.9383535897575734 and prediction 0.9687866277253536.\n",
      "The error is 0.0009500699739443887 for weight 1.9391241698856039 and prediction 0.9691767948787867.\n",
      "The error is 0.0009264666730292044 for weight 1.9398851177620338 and prediction 0.9695620849428019.\n",
      "The error is 0.0009034497666211372 for weight 1.9406365537900083 and prediction 0.9699425588810169.\n",
      "The error is 0.0008810046864816438 for weight 1.9413785968676331 and prediction 0.9703182768950042.\n",
      "The error is 0.0008591172263018677 for weight 1.9421113644067878 and prediction 0.9706892984338166.\n",
      "The error is 0.0008377735327109293 for weight 1.9428349723517029 and prediction 0.9710556822033939.\n",
      "The error is 0.0008169600965076435 for weight 1.9435495351973067 and prediction 0.9714174861758514.\n",
      "The error is 0.0007966637441100298 for weight 1.9442551660073404 and prediction 0.9717747675986533.\n",
      "The error is 0.0007768716292172947 for weight 1.9449519764322487 and prediction 0.9721275830036702.\n",
      "The error is 0.0007575712246789254 for weight 1.9456400767268456 and prediction 0.9724759882161244.\n",
      "The error is 0.0007387503145658089 for weight 1.94631957576776 and prediction 0.9728200383634228.\n",
      "The error is 0.0007203969864383136 for weight 1.9469905810706631 and prediction 0.97315978788388.\n",
      "The error is 0.0007024996238064842 for weight 1.9476531988072798 and prediction 0.9734952905353316.\n",
      "The error is 0.0006850468987775425 for weight 1.948307533822189 and prediction 0.9738265994036399.\n",
      "The error is 0.0006680277648860359 for weight 1.9489536896494115 and prediction 0.9741537669110945.\n",
      "The error is 0.0006514314501021493 for weight 1.9495917685287938 and prediction 0.9744768448247058.\n",
      "The error is 0.0006352474500136761 for weight 1.950221871422184 and prediction 0.9747958842643969.\n",
      "The error is 0.0006194655211773967 for weight 1.9508440980294066 and prediction 0.975110935711092.\n",
      "The error is 0.0006040756746356466 for weight 1.951458546804039 and prediction 0.9754220490147033.\n",
      "The error is 0.0005890681695939163 for weight 1.9520653149689886 and prediction 0.9757292734020195.\n",
      "The error is 0.0005744335072555669 for weight 1.9526644985318762 and prediction 0.9760326574844943.\n",
      "The error is 0.0005601624248096881 for weight 1.9532561923002276 and prediction 0.9763322492659381.\n",
      "The error is 0.0005462458895683248 for weight 1.9538404898964747 and prediction 0.9766280961501138.\n",
      "The error is 0.0005326750932493626 for weight 1.9544174837727688 and prediction 0.9769202449482374.\n",
      "The error is 0.0005194414464014488 for weight 1.9549872652256093 and prediction 0.9772087418863844.\n",
      "The error is 0.0005065365729674109 for weight 1.9555499244102892 and prediction 0.9774936326128046.\n",
      "The error is 0.0004939523049827513 for weight 1.9561055503551605 and prediction 0.9777749622051446.\n",
      "The error is 0.0004816806774058368 for weight 1.956654230975721 and prediction 0.9780527751775803.\n",
      "The error is 0.00046971392307653385 for weight 1.9571960530885246 and prediction 0.9783271154878606.\n",
      "The error is 0.0004580444678001006 for weight 1.957731102424918 and prediction 0.9785980265442623.\n",
      "The error is 0.00044666492555319343 for weight 1.9582594636446065 and prediction 0.978865551212459.\n",
      "The error is 0.0004355680938089825 for weight 1.958781220349049 and prediction 0.9791297318223032.\n",
      "The error is 0.0004247469489784153 for weight 1.9592964550946859 and prediction 0.9793906101745244.\n",
      "The error is 0.0004141946419647311 for weight 1.9598052494060023 and prediction 0.9796482275473429.\n",
      "The error is 0.0004039044938284193 for weight 1.9603076837884272 and prediction 0.9799026247030012.\n",
      "The error is 0.00039386999155987054 for weight 1.960803837741072 and prediction 0.9801538418942136.\n",
      "The error is 0.0003840847839570545 for weight 1.9612937897693086 and prediction 0.980401918870536.\n",
      "The error is 0.00037454267760562044 for weight 1.9617776173971921 and prediction 0.9806468948846543.\n",
      "The error is 0.00036523763295885723 for weight 1.9622553971797272 and prediction 0.9808888086985961.\n",
      "The error is 0.0003561637605150365 for weight 1.9627272047149806 and prediction 0.9811276985898636.\n",
      "The error is 0.0003473153170897405 for weight 1.9631931146560433 and prediction 0.9813636023574903.\n",
      "The error is 0.00033868670218079375 for weight 1.9636532007228427 and prediction 0.9815965573280216.\n",
      "The error is 0.0003302724544234913 for weight 1.9641075357138071 and prediction 0.9818266003614213.\n",
      "The error is 0.0003220672481339075 for weight 1.9645561915173846 and prediction 0.9820537678569036.\n",
      "The error is 0.0003140658899380801 for weight 1.9649992391234172 and prediction 0.9822780957586923.\n",
      "The error is 0.0003062633154849318 for weight 1.9654367486343745 and prediction 0.9824996195617086.\n",
      "The error is 0.00029865458624085407 for weight 1.9658687892764448 and prediction 0.9827183743171872.\n",
      "The error is 0.0002912348863639321 for weight 1.9662954294104893 and prediction 0.9829343946382224.\n",
      "The error is 0.00028399951965582754 for weight 1.9667167365428582 and prediction 0.9831477147052446.\n",
      "The error is 0.0002769439065893782 for weight 1.9671327773360725 and prediction 0.9833583682714291.\n",
      "The error is 0.00027006358141004735 for weight 1.9675436176193717 and prediction 0.9835663886680363.\n",
      "The error is 0.00026335418930939 for weight 1.9679493223991296 and prediction 0.9837718088096858.\n",
      "The error is 0.0002568114836687345 for weight 1.9683499558691404 and prediction 0.9839746611995648.\n",
      "The error is 0.00025043132337133987 for weight 1.9687455814207762 and prediction 0.9841749779345702.\n",
      "The error is 0.0002442096701813331 for weight 1.9691362616530164 and prediction 0.9843727907103881.\n",
      "The error is 0.00023814258618776708 for weight 1.9695220583823536 and prediction 0.9845681308265082.\n",
      "The error is 0.00023222623131216542 for weight 1.9699030326525742 and prediction 0.9847610291911768.\n",
      "The error is 0.00022645686087800431 for weight 1.970279244744417 and prediction 0.9849515163262871.\n",
      "The error is 0.0002208308232405651 for weight 1.970650754185112 and prediction 0.9851396223722085.\n",
      "The error is 0.00021534455747568122 for weight 1.971017619757798 and prediction 0.985325377092556.\n",
      "The error is 0.00020999459112589538 for weight 1.9713798995108256 and prediction 0.985508809878899.\n",
      "The error is 0.00020477753800261024 for weight 1.9717376507669402 and prediction 0.9856899497554128.\n",
      "The error is 0.00019969009604285917 for weight 1.9720909301323535 and prediction 0.9858688253834701.\n",
      "The error is 0.0001947290452192937 for weight 1.972439793505699 and prediction 0.9860454650661767.\n",
      "The error is 0.00018989124550212804 for weight 1.9727842960868778 and prediction 0.9862198967528495.\n",
      "The error is 0.0001851736348716841 for weight 1.9731244923857918 and prediction 0.9863921480434389.\n",
      "The error is 0.0001805732273803408 for weight 1.9734604362309693 and prediction 0.9865622461928959.\n",
      "The error is 0.00017608711126261122 for weight 1.9737921807780823 and prediction 0.9867302181154847.\n",
      "The error is 0.0001717124470921801 for weight 1.9741197785183562 and prediction 0.9868960903890411.\n",
      "The error is 0.00016744646598473452 for weight 1.9744432812868766 and prediction 0.9870598892591781.\n",
      "The error is 0.00016328646784542758 for weight 1.9747627402707906 and prediction 0.9872216406434383.\n",
      "The error is 0.00015922981965989396 for weight 1.9750782060174057 and prediction 0.9873813701353953.\n",
      "The error is 0.00015527395382771848 for weight 1.9753897284421882 and prediction 0.9875391030087028.\n",
      "The error is 0.00015141636653731027 for weight 1.975697356836661 and prediction 0.9876948642210941.\n",
      "The error is 0.0001476546161811479 for weight 1.9760011398762027 and prediction 0.9878486784183305.\n",
      "The error is 0.00014398632181039658 for weight 1.9763011256277503 and prediction 0.9880005699381014.\n",
      "The error is 0.00014040916162791855 for weight 1.9765973615574035 and prediction 0.9881505628138751.\n",
      "The error is 0.00013692087151872376 for weight 1.976889894537936 and prediction 0.9882986807787018.\n",
      "The error is 0.00013351924361692933 for weight 1.9771787708562119 and prediction 0.988444947268968.\n",
      "The error is 0.00013020212490832137 for weight 1.9774640362205091 and prediction 0.9885893854281059.\n",
      "The error is 0.0001269674158676309 for weight 1.9777457357677528 and prediction 0.9887320181102546.\n",
      "The error is 0.00012381306912966915 for weight 1.978023914070656 and prediction 0.9888728678838764.\n",
      "The error is 0.00012073708819347872 for weight 1.9782986151447728 and prediction 0.989011957035328.\n",
      "The error is 0.00011773752615867125 for weight 1.9785698824554632 and prediction 0.9891493075723864.\n",
      "The error is 0.00011481248449316633 for weight 1.9788377589247699 and prediction 0.9892849412277316.\n",
      "The error is 0.00011196011183153927 for weight 1.9791022869382102 and prediction 0.9894188794623849.\n",
      "The error is 0.0001091786028032248 for weight 1.9793635083514827 and prediction 0.9895511434691051.\n",
      "The error is 0.00010646619688983138 for weight 1.979621464497089 and prediction 0.9896817541757413.\n",
      "The error is 0.0001038211773108501 for weight 1.9798761961908755 and prediction 0.9898107322485445.\n",
      "The error is 0.00010124186993703329 for weight 1.9801277437384897 and prediction 0.9899380980954378.\n",
      "The error is 9.872664223078429e-05 for weight 1.9803761469417585 and prediction 0.9900638718692448.\n",
      "The error is 9.627390221286403e-05 for weight 1.9806214451049864 and prediction 0.9901880734708792.\n",
      "The error is 9.388209745476399e-05 for weight 1.980863677041174 and prediction 0.9903107225524932.\n",
      "The error is 9.154971409612225e-05 for weight 1.9811028810781595 and prediction 0.990431838520587.\n",
      "The error is 8.927527588654594e-05 for weight 1.9813390950646825 and prediction 0.9905514405390797.\n",
      "The error is 8.705734325123896e-05 for weight 1.981572356376374 and prediction 0.9906695475323413.\n",
      "The error is 8.489451237984027e-05 for weight 1.9818027019216693 and prediction 0.990786178188187.\n",
      "The error is 8.278541433790458e-05 for weight 1.9820301681476484 and prediction 0.9909013509608346.\n",
      "The error is 8.072871420044727e-05 for weight 1.9822547910458028 and prediction 0.9910150840738242.\n",
      "The error is 7.872311020702993e-05 for weight 1.9824766061577304 and prediction 0.9911273955229014.\n",
      "The error is 7.67673329378234e-05 for weight 1.9826956485807588 and prediction 0.9912383030788652.\n",
      "The error is 7.486014451014915e-05 for weight 1.9829119529734993 and prediction 0.9913478242903794.\n",
      "The error is 7.300033779497473e-05 for weight 1.9831255535613306 and prediction 0.9914559764867497.\n",
      "The error is 7.118673565288078e-05 for weight 1.983336484141814 and prediction 0.9915627767806653.\n",
      "The error is 6.94181901890044e-05 for weight 1.9835447780900413 and prediction 0.991668242070907.\n",
      "The error is 6.769358202649654e-05 for weight 1.9837504683639158 and prediction 0.9917723890450206.\n",
      "The error is 6.601181959802552e-05 for weight 1.9839535875093668 and prediction 0.9918752341819579.\n",
      "The error is 6.437183845488739e-05 for weight 1.9841541676654997 and prediction 0.9919767937546834.\n",
      "The error is 6.277260059327397e-05 for weight 1.984352240569681 and prediction 0.9920770838327498.\n",
      "The error is 6.121309379728462e-05 for weight 1.98454783756256 and prediction 0.9921761202848405.\n",
      "The error is 5.969233099825765e-05 for weight 1.9847409895930281 and prediction 0.99227391878128.\n",
      "The error is 5.8209349650018904e-05 for weight 1.9849317272231153 and prediction 0.9923704947965141.\n",
      "The error is 5.676321111965127e-05 for weight 1.9851200806328264 and prediction 0.9924658636115576.\n",
      "The error is 5.535300009339691e-05 for weight 1.985306079624916 and prediction 0.9925600403164132.\n",
      "The error is 5.397782399732711e-05 for weight 1.9854897536296046 and prediction 0.992653039812458.\n",
      "The error is 5.263681243239352e-05 for weight 1.9856711317092346 and prediction 0.9927448768148023.\n",
      "The error is 5.1329116623525584e-05 for weight 1.9858502425628692 and prediction 0.9928355658546173.\n",
      "The error is 5.005390888240932e-05 for weight 1.9860271145308335 and prediction 0.9929251212814346.\n",
      "The error is 4.8810382083611286e-05 for weight 1.986201775599198 and prediction 0.9930135572654167.\n",
      "The error is 4.7597749153721356e-05 for weight 1.9863742534042081 and prediction 0.993100887799599.\n",
      "The error is 4.641524257318333e-05 for weight 1.9865445752366555 and prediction 0.9931871267021041.\n",
      "The error is 4.526211389050605e-05 for weight 1.9867127680461973 and prediction 0.9932722876183278.\n",
      "The error is 4.41376332485386e-05 for weight 1.9868788584456198 and prediction 0.9933563840230987.\n",
      "The error is 4.304108892252042e-05 for weight 1.9870428727150495 and prediction 0.9934394292228099.\n",
      "The error is 4.197178686960195e-05 for weight 1.9872048368061115 and prediction 0.9935214363575248.\n",
      "The error is 4.092905028955984e-05 for weight 1.9873647763460351 and prediction 0.9936024184030557.\n",
      "The error is 3.991221919642825e-05 for weight 1.9875227166417098 and prediction 0.9936823881730176.\n",
      "The error is 3.892065000076662e-05 for weight 1.9876786826836883 and prediction 0.9937613583208549.\n",
      "The error is 3.795371510231062e-05 for weight 1.9878326991501423 and prediction 0.9938393413418442.\n",
      "The error is 3.701080249273717e-05 for weight 1.9879847904107655 and prediction 0.9939163495750711.\n",
      "The error is 3.6091315368308295e-05 for weight 1.988134980530631 and prediction 0.9939923952053827.\n",
      "The error is 3.5194671752126474e-05 for weight 1.988283293273998 and prediction 0.9940674902653155.\n",
      "The error is 3.432030412578514e-05 for weight 1.988429752108073 and prediction 0.994141646636999.\n",
      "The error is 3.346765907016068e-05 for weight 1.988574380206722 and prediction 0.9942148760540365.\n",
      "The error is 3.26361969151368e-05 for weight 1.988717200454138 and prediction 0.994287190103361.\n",
      "The error is 3.182539139802581e-05 for weight 1.9888582354484612 and prediction 0.994358600227069.\n",
      "The error is 3.1034729330481724e-05 for weight 1.9889975075053554 and prediction 0.9944291177242306.\n",
      "The error is 3.0263710273677565e-05 for weight 1.9891350386615385 and prediction 0.9944987537526777.\n",
      "The error is 2.951184622156571e-05 for weight 1.9892708506782693 and prediction 0.9945675193307693.\n",
      "The error is 2.8778661291998836e-05 for weight 1.989404965044791 and prediction 0.9946354253391346.\n",
      "The error is 2.8063691425525577e-05 for weight 1.989537402981731 and prediction 0.9947024825223955.\n",
      "The error is 2.736648409167292e-05 for weight 1.9896681854444593 and prediction 0.9947687014908655.\n",
      "The error is 2.668659800252048e-05 for weight 1.9897973331264036 and prediction 0.9948340927222297.\n",
      "The error is 2.6023602833395278e-05 for weight 1.9899248664623235 and prediction 0.9948986665632018.\n",
      "The error is 2.537707895050359e-05 for weight 1.9900508056315445 and prediction 0.9949624332311617.\n",
      "The error is 2.474661714532685e-05 for weight 1.9901751705611501 and prediction 0.9950254028157722.\n",
      "The error is 2.4131818375622813e-05 for weight 1.9902979809291357 and prediction 0.9950875852805751.\n",
      "The error is 2.3532293512853742e-05 for weight 1.9904192561675216 and prediction 0.9951489904645678.\n",
      "The error is 2.294766309589333e-05 for weight 1.9905390154654277 and prediction 0.9952096280837608.\n",
      "The error is 2.237755709085423e-05 for weight 1.9906572777721099 and prediction 0.9952695077327138.\n",
      "The error is 2.1821614656878038e-05 for weight 1.9907740617999585 and prediction 0.9953286388860549.\n",
      "The error is 2.1279483917746123e-05 for weight 1.990889386027459 and prediction 0.9953870308999793.\n",
      "The error is 2.0750821739164743e-05 for weight 1.9910032687021157 and prediction 0.9954446930137295.\n",
      "The error is 2.023529351158252e-05 for weight 1.9911157278433393 and prediction 0.9955016343510579.\n",
      "The error is 1.973257293840394e-05 for weight 1.9912267812452975 and prediction 0.9955578639216697.\n",
      "The error is 1.9242341829465896e-05 for weight 1.9913364464797312 and prediction 0.9956133906226488.\n",
      "The error is 1.876428989964056e-05 for weight 1.9914447408987346 and prediction 0.9956682232398656.\n",
      "The error is 1.829811457244621e-05 for weight 1.9915516816375003 and prediction 0.9957223704493673.\n",
      "The error is 1.7843520788537432e-05 for weight 1.9916572856170316 and prediction 0.9957758408187501.\n",
      "The error is 1.7400220818946986e-05 for weight 1.9917615695468187 and prediction 0.9958286428085158.\n",
      "The error is 1.6967934082976215e-05 for weight 1.9918645499274834 and prediction 0.9958807847734094.\n",
      "The error is 1.654638697060256e-05 for weight 1.9919662430533898 and prediction 0.9959322749637417.\n",
      "The error is 1.613531266930206e-05 for weight 1.9920666650152223 and prediction 0.9959831215266949.\n",
      "The error is 1.5734450995174297e-05 for weight 1.9921658317025321 and prediction 0.9960333325076112.\n",
      "The error is 1.534354822826261e-05 for weight 1.9922637588062504 and prediction 0.9960829158512661.\n",
      "The error is 1.4962356951967076e-05 for weight 1.9923604618211723 and prediction 0.9961318794031252.\n",
      "The error is 1.4590635896441422e-05 for weight 1.9924559560484076 and prediction 0.9961802309105862.\n",
      "The error is 1.4228149785889321e-05 for weight 1.9925502565978026 and prediction 0.9962279780242038.\n",
      "The error is 1.3874669189645904e-05 for weight 1.9926433783903301 and prediction 0.9962751282989013.\n",
      "The error is 1.3529970376965456e-05 for weight 1.992735336160451 and prediction 0.9963216891951651.\n",
      "The error is 1.3193835175412871e-05 for weight 1.9928261444584454 and prediction 0.9963676680802255.\n",
      "The error is 1.286605083277336e-05 for weight 1.9929158176527149 and prediction 0.9964130722292227.\n",
      "The error is 1.2546409882396597e-05 for weight 1.9930043699320559 and prediction 0.9964579088263574.\n",
      "The error is 1.2234710011880981e-05 for weight 1.9930918153079051 and prediction 0.9965021849660279.\n",
      "The error is 1.1930753935023457e-05 for weight 1.9931781676165563 and prediction 0.9965459076539526.\n",
      "The error is 1.1634349266950265e-05 for weight 1.9932634405213494 and prediction 0.9965890838082782.\n",
      "The error is 1.1345308402349293e-05 for weight 1.9933476475148326 and prediction 0.9966317202606747.\n",
      "The error is 1.106344839672815e-05 for weight 1.9934308019208973 and prediction 0.9966738237574163.\n",
      "The error is 1.0788590850621726e-05 for weight 1.9935129168968861 and prediction 0.9967154009604486.\n",
      "The error is 1.0520561796676322e-05 for weight 1.993594005435675 and prediction 0.9967564584484431.\n",
      "The error is 1.025919158954001e-05 for weight 1.993674080367729 and prediction 0.9967970027178376.\n",
      "The error is 1.000431479848765e-05 for weight 1.9937531543631324 and prediction 0.9968370401838645.\n",
      "The error is 9.755770102712989e-06 for weight 1.9938312399335931 and prediction 0.9968765771815662.\n",
      "The error is 9.51340018922404e-06 for weight 1.9939083494344232 and prediction 0.9969156199667966.\n",
      "The error is 9.277051653273072e-06 for weight 1.993984495066493 and prediction 0.9969541747172116.\n",
      "The error is 9.046574901262095e-06 for weight 1.9940596888781617 and prediction 0.9969922475332464.\n",
      "The error is 8.821824056059095e-06 for weight 1.9941339427671847 and prediction 0.9970298444390808.\n",
      "The error is 8.602656864666182e-06 for weight 1.9942072684825949 and prediction 0.9970669713835923.\n",
      "The error is 8.388934608184638e-06 for weight 1.9942796776265626 and prediction 0.9971036342412974.\n",
      "The error is 8.180522014012259e-06 for weight 1.9943511816562305 and prediction 0.9971398388132813.\n",
      "The error is 7.977287170226736e-06 for weight 1.9944217918855276 and prediction 0.9971755908281152.\n",
      "The error is 7.779101442091446e-06 for weight 1.9944915194869586 and prediction 0.9972108959427638.\n",
      "The error is 7.58583939063928e-06 for weight 1.9945603754933716 and prediction 0.9972457597434793.\n",
      "The error is 7.397378693278018e-06 for weight 1.9946283707997046 and prediction 0.9972801877466858.\n",
      "The error is 7.213600066366646e-06 for weight 1.9946955161647082 and prediction 0.9973141853998523.\n",
      "The error is 7.034387189718019e-06 for weight 1.9947618222126493 and prediction 0.9973477580823541.\n",
      "The error is 6.859626632973556e-06 for weight 1.9948272994349912 and prediction 0.9973809111063247.\n",
      "The error is 6.689207783810712e-06 for weight 1.9948919581920537 and prediction 0.9974136497174956.\n",
      "The error is 6.523022777931899e-06 for weight 1.994955808714653 and prediction 0.9974459790960268.\n",
      "The error is 6.360966430792514e-06 for weight 1.9950188611057198 and prediction 0.9974779043573265.\n",
      "The error is 6.2029361710277475e-06 for weight 1.9950811253418983 and prediction 0.9975094305528599.\n",
      "The error is 6.048831975528783e-06 for weight 1.9951426112751245 and prediction 0.9975405626709491.\n",
      "The error is 5.898556306136936e-06 for weight 1.9952033286341855 and prediction 0.9975713056375622.\n",
      "The error is 5.752014047906233e-06 for weight 1.995263287026258 and prediction 0.9976016643170927.\n",
      "The error is 5.609112448903701e-06 for weight 1.99532249593843 and prediction 0.997631643513129.\n",
      "The error is 5.469761061501152e-06 for weight 1.9953809647391996 and prediction 0.997661247969215.\n",
      "The error is 5.333871685129425e-06 for weight 1.9954387026799596 and prediction 0.9976904823695998.\n",
      "The error is 5.20135831045194e-06 for weight 1.9954957188964602 and prediction 0.9977193513399798.\n",
      "The error is 5.072137064926524e-06 for weight 1.9955520224102543 and prediction 0.9977478594482301.\n",
      "The error is 4.946126159719967e-06 for weight 1.9956076221301262 and prediction 0.9977760112051272.\n",
      "The error is 4.823245837939247e-06 for weight 1.9956625268534995 and prediction 0.9978038110650631.\n",
      "The error is 4.703418324153148e-06 for weight 1.9957167452678308 and prediction 0.9978312634267498.\n",
      "The error is 4.586567775162403e-06 for weight 1.995770285951983 and prediction 0.9978583726339154.\n",
      "The error is 4.472620231998036e-06 for weight 1.9958231573775833 and prediction 0.9978851429759915.\n",
      "The error is 4.361503573109178e-06 for weight 1.9958753679103636 and prediction 0.9979115786887917.\n",
      "The error is 4.253147468714638e-06 for weight 1.995926925811484 and prediction 0.9979376839551818.\n",
      "The error is 4.147483336288799e-06 for weight 1.9959778392388405 and prediction 0.997963462905742.\n",
      "The error is 4.044444297152723e-06 for weight 1.996028116248355 and prediction 0.9979889196194203.\n",
      "The error is 3.9439651341452354e-06 for weight 1.9960777647952506 and prediction 0.9980140581241775.\n",
      "The error is 3.845982250343962e-06 for weight 1.99612679273531 and prediction 0.9980388823976253.\n",
      "The error is 3.7504336288120278e-06 for weight 1.9961752078261186 and prediction 0.998063396367655.\n",
      "The error is 3.65725879334616e-06 for weight 1.996223017728292 and prediction 0.9980876039130593.\n",
      "The error is 3.5663987701990814e-06 for weight 1.9962702300066884 and prediction 0.998111508864146.\n",
      "The error is 3.477796050751995e-06 for weight 1.9963168521316048 and prediction 0.9981351150033442.\n",
      "The error is 3.391394555116002e-06 for weight 1.9963628914799598 and prediction 0.9981584260658024.\n",
      "The error is 3.307139596637213e-06 for weight 1.9964083553364602 and prediction 0.9981814457399799.\n",
      "The error is 3.2249778472834514e-06 for weight 1.9964532508947546 and prediction 0.9982041776682301.\n",
      "The error is 3.144857303889831e-06 for weight 1.9964975852585702 and prediction 0.9982266254473773.\n",
      "The error is 3.0667272552461233e-06 for weight 1.9965413654428381 and prediction 0.9982487926292851.\n",
      "The error is 2.9905382499986024e-06 for weight 1.9965845983748027 and prediction 0.9982706827214191.\n",
      "The error is 2.916242065350034e-06 for weight 1.9966272908951177 and prediction 0.9982922991874014.\n",
      "The error is 2.843791676538989e-06 for weight 1.9966694497589288 and prediction 0.9983136454475589.\n",
      "The error is 2.7731412270748675e-06 for weight 1.9967110816369422 and prediction 0.9983347248794644.\n",
      "The error is 2.704245999714667e-06 for weight 1.9967521931164804 and prediction 0.9983555408184711.\n",
      "The error is 2.637062388159251e-06 for weight 1.9967927907025245 and prediction 0.9983760965582402.\n",
      "The error is 2.5715478694533754e-06 for weight 1.996832880818743 and prediction 0.9983963953512622.\n",
      "The error is 2.507660977071511e-06 for weight 1.9968724698085087 and prediction 0.9984164404093715.\n",
      "The error is 2.4453612746723344e-06 for weight 1.9969115639359023 and prediction 0.9984362349042544.\n",
      "The error is 2.384609330504822e-06 for weight 1.9969501693867036 and prediction 0.9984557819679512.\n",
      "The error is 2.3253666924500423e-06 for weight 1.9969882922693698 and prediction 0.9984750846933518.\n",
      "The error is 2.267595863684424e-06 for weight 1.9970259386160027 and prediction 0.9984941461346849.\n",
      "The error is 2.2112602789459105e-06 for weight 1.9970631143833026 and prediction 0.9985129693080014.\n",
      "The error is 2.156324281390958e-06 for weight 1.9970998254535113 and prediction 0.9985315571916513.\n",
      "The error is 2.102753100025264e-06 for weight 1.9971360776353424 and prediction 0.9985499127267556.\n",
      "The error is 2.050512827696496e-06 for weight 1.9971718766649007 and prediction 0.9985680388176712.\n",
      "The error is 1.9995703996332933e-06 for weight 1.9972072282065894 and prediction 0.9985859383324504.\n",
      "The error is 1.9498935725175237e-06 for weight 1.997242137854007 and prediction 0.9986036141032947.\n",
      "The error is 1.9014509040751957e-06 for weight 1.997276611130832 and prediction 0.9986210689270035.\n",
      "The error is 1.8542117331770511e-06 for weight 1.9973106534916967 and prediction 0.998638305565416.\n",
      "The error is 1.8081461604308218e-06 for weight 1.9973442703230504 and prediction 0.9986553267458483.\n",
      "The error is 1.7632250292577108e-06 for weight 1.9973774669440123 and prediction 0.9986721351615252.\n",
      "The error is 1.7194199074370422e-06 for weight 1.997410248607212 and prediction 0.9986887334720062.\n",
      "The error is 1.6767030691117717e-06 for weight 1.9974426204996218 and prediction 0.998705124303606.\n",
      "The error is 1.6350474772386043e-06 for weight 1.9974745877433766 and prediction 0.9987213102498109.\n",
      "The error is 1.5944267664759157e-06 for weight 1.9975061553965845 and prediction 0.9987372938716883.\n",
      "The error is 1.554815226496162e-06 for weight 1.9975373284541271 and prediction 0.9987530776982922.\n",
      "The error is 1.516187785712973e-06 for weight 1.9975681118484505 and prediction 0.9987686642270636.\n",
      "The error is 1.4785199954116969e-06 for weight 1.9975985104503449 and prediction 0.9987840559242253.\n",
      "The error is 1.4417880142757042e-06 for weight 1.9976285290697156 and prediction 0.9987992552251724.\n",
      "The error is 1.4059685932960453e-06 for weight 1.997658172456344 and prediction 0.9988142645348578.\n",
      "The error is 1.371039061056441e-06 for weight 1.9976874453006397 and prediction 0.998829086228172.\n",
      "The error is 1.3369773093833682e-06 for weight 1.9977163522343817 and prediction 0.9988437226503198.\n",
      "The error is 1.303761779353353e-06 for weight 1.997744897831452 and prediction 0.9988581761171909.\n",
      "The error is 1.2713714476474304e-06 for weight 1.9977730866085588 and prediction 0.998872448915726.\n",
      "The error is 1.2397858132450478e-06 for weight 1.997800923025952 and prediction 0.9988865433042794.\n",
      "The error is 1.208984884447122e-06 for weight 1.9978284114881275 and prediction 0.998900461512976.\n",
      "The error is 1.178949166224145e-06 for weight 1.997855556344526 and prediction 0.9989142057440638.\n",
      "The error is 1.149659647875737e-06 for weight 1.9978823618902193 and prediction 0.998927778172263.\n",
      "The error is 1.1210977909989123e-06 for weight 1.9979088323665914 and prediction 0.9989411809451096.\n",
      "The error is 1.0932455177538818e-06 for weight 1.997934971962009 and prediction 0.9989544161832957.\n",
      "The error is 1.066085199422264e-06 for weight 1.9979607848124838 and prediction 0.9989674859810045.\n",
      "The error is 1.039599645249134e-06 for weight 1.9979862750023278 and prediction 0.9989803924062419.\n",
      "The error is 1.0137720915624369e-06 for weight 1.9980114465647987 and prediction 0.9989931375011639.\n",
      "The error is 9.88586191162746e-07 for weight 1.9980363034827386 and prediction 0.9990057232823993.\n",
      "The error is 9.640260029760984e-07 for weight 1.9980608496892045 and prediction 0.9990181517413693.\n",
      "The error is 9.40075981964572e-07 for weight 1.9980850890680895 and prediction 0.9990304248446022.\n",
      "The error is 9.167209692876238e-07 for weight 1.9981090254547382 and prediction 0.9990425445340447.\n",
      "The error is 8.939461827069893e-07 for weight 1.998132662636554 and prediction 0.9990545127273691.\n",
      "The error is 8.717372072302797e-07 for weight 1.998156004353597 and prediction 0.999066331318277.\n",
      "The error is 8.500799859882292e-07 for weight 1.998179054299177 and prediction 0.9990780021767985.\n",
      "The error is 8.289608113363669e-07 for weight 1.9982018161204373 and prediction 0.9990895271495885.\n",
      "The error is 8.08366316179749e-07 for weight 1.9982242934189318 and prediction 0.9991009080602187.\n",
      "The error is 7.882834655122127e-07 for weight 1.9982464897511951 and prediction 0.9991121467094659.\n",
      "The error is 7.686995481659374e-07 for weight 1.9982684086293052 and prediction 0.9991232448755976.\n",
      "The error is 7.49602168766161e-07 for weight 1.998290053521439 and prediction 0.9991342043146526.\n",
      "The error is 7.309792398858269e-07 for weight 1.998311427852421 and prediction 0.9991450267607195.\n",
      "The error is 7.128189743948924e-07 for weight 1.9983325350042658 and prediction 0.9991557139262105.\n",
      "The error is 6.951098779997345e-07 for weight 1.9983533783167124 and prediction 0.9991662675021329.\n",
      "The error is 6.778407419681832e-07 for weight 1.9983739610877536 and prediction 0.9991766891583562.\n",
      "The error is 6.610006360348975e-07 for weight 1.9983942865741566 and prediction 0.9991869805438768.\n",
      "The error is 6.445789014834122e-07 for weight 1.9984143579919798 and prediction 0.9991971432870783.\n",
      "The error is 6.285651443996221e-07 for weight 1.99843417851708 and prediction 0.9992071789959899.\n",
      "The error is 6.12949229093407e-07 for weight 1.9984537512856166 and prediction 0.99921708925854.\n",
      "The error is 5.977212716830469e-07 for weight 1.9984730793945464 and prediction 0.9992268756428083.\n",
      "The error is 5.828716338396691e-07 for weight 1.9984921659021146 and prediction 0.9992365396972732.\n",
      "The error is 5.683909166864334e-07 for weight 1.9985110138283382 and prediction 0.9992460829510573.\n",
      "The error is 5.542699548500318e-07 for weight 1.998529626155484 and prediction 0.9992555069141691.\n",
      "The error is 5.404998106592589e-07 for weight 1.9985480058285403 and prediction 0.999264813077742.\n",
      "The error is 5.270717684882292e-07 for weight 1.9985661557556835 and prediction 0.9992740029142702.\n",
      "The error is 5.139773292398697e-07 for weight 1.9985840788087375 and prediction 0.9992830778778418.\n",
      "The error is 5.012082049665882e-07 for weight 1.9986017778236282 and prediction 0.9992920394043687.\n",
      "The error is 4.887563136244418e-07 for weight 1.9986192556008329 and prediction 0.9993008889118141.\n",
      "The error is 4.7661377395785186e-07 for weight 1.9986365149058225 and prediction 0.9993096278004164.\n",
      "The error is 4.647729005110543e-07 for weight 1.9986535584694998 and prediction 0.9993182574529113.\n",
      "The error is 4.5322619876395105e-07 for weight 1.998670388988631 and prediction 0.9993267792347499.\n",
      "The error is 4.419663603884036e-07 for weight 1.9986870091262732 and prediction 0.9993351944943155.\n",
      "The error is 4.3098625862248414e-07 for weight 1.9987034215121948 and prediction 0.9993435045631366.\n",
      "The error is 4.202789437597904e-07 for weight 1.9987196287432925 and prediction 0.9993517107560974.\n",
      "The error is 4.098376387507155e-07 for weight 1.9987356333840014 and prediction 0.9993598143716462.\n",
      "The error is 3.996557349129392e-07 for weight 1.9987514379667013 and prediction 0.9993678166920007.\n",
      "The error is 3.8972678774874957e-07 for weight 1.9987670449921175 and prediction 0.9993757189833506.\n",
      "The error is 3.8004451286560464e-07 for weight 1.9987824569297161 and prediction 0.9993835224960588.\n",
      "The error is 3.7060278199907274e-07 for weight 1.9987976762180946 and prediction 0.9993912284648581.\n",
      "The error is 3.6139561913384665e-07 for weight 1.9988127052653684 and prediction 0.9993988381090473.\n",
      "The error is 3.5241719672100336e-07 for weight 1.9988275464495513 and prediction 0.9994063526326842.\n",
      "The error is 3.4366183198994315e-07 for weight 1.9988422021189318 and prediction 0.9994137732247756.\n",
      "The error is 3.351239833514912e-07 for weight 1.9988566745924452 and prediction 0.9994211010594659.\n",
      "The error is 3.267982468900851e-07 for weight 1.9988709661600397 and prediction 0.9994283372962226.\n",
      "The error is 3.1867935294388923e-07 for weight 1.9988850790830393 and prediction 0.9994354830800198.\n",
      "The error is 3.1076216276914924e-07 for weight 1.9988990155945012 and prediction 0.9994425395415196.\n",
      "The error is 3.030416652878715e-07 for weight 1.99891277789957 and prediction 0.9994495077972506.\n",
      "The error is 2.955129739159137e-07 for weight 1.9989263681758254 and prediction 0.999456388949785.\n",
      "The error is 2.8817132347013955e-07 for weight 1.9989397885736275 and prediction 0.9994631840879127.\n",
      "The error is 2.810120671526871e-07 for weight 1.9989530412164571 and prediction 0.9994698942868138.\n",
      "The error is 2.7403067360939884e-07 for weight 1.9989661282012514 and prediction 0.9994765206082286.\n",
      "The error is 2.672227240619096e-07 for weight 1.9989790515987358 and prediction 0.9994830641006257.\n",
      "The error is 2.605839095109866e-07 for weight 1.9989918134537517 and prediction 0.9994895257993679.\n",
      "The error is 2.541100280090423e-07 for weight 1.9990044157855797 and prediction 0.9994959067268758.\n",
      "The error is 2.4779698200069816e-07 for weight 1.99901686058826 and prediction 0.9995022078927899.\n",
      "The error is 2.4164077572917153e-07 for weight 1.9990291498309067 and prediction 0.99950843029413.\n",
      "The error is 2.356375127071041e-07 for weight 1.9990412854580204 and prediction 0.9995145749154534.\n",
      "The error is 2.297833932507883e-07 for weight 1.999053269389795 and prediction 0.9995206427290102.\n",
      "The error is 2.2407471207473243e-07 for weight 1.9990651035224227 and prediction 0.9995266346948976.\n",
      "The error is 2.1850785594659854e-07 for weight 1.9990767897283925 and prediction 0.9995325517612114.\n",
      "The error is 2.1307930140038295e-07 for weight 1.9990883298567876 and prediction 0.9995383948641963.\n",
      "The error is 2.0778561250621214e-07 for weight 1.9990997257335779 and prediction 0.9995441649283938.\n",
      "The error is 2.0262343869548094e-07 for weight 1.9991109791619082 and prediction 0.9995498628667889.\n",
      "The error is 1.9758951264037775e-07 for weight 1.9991220919223842 and prediction 0.9995554895809541.\n",
      "The error is 1.9268064818576102e-07 for weight 1.9991330657733544 and prediction 0.9995610459611921.\n",
      "The error is 1.8789373833241525e-07 for weight 1.9991439024511874 and prediction 0.9995665328866772.\n",
      "The error is 1.8322575327071457e-07 for weight 1.9991546036705476 and prediction 0.9995719512255937.\n",
      "The error is 1.7867373846287765e-07 for weight 1.9991651711246659 and prediction 0.9995773018352738.\n",
      "The error is 1.7423481277291969e-07 for weight 1.9991756064856074 and prediction 0.9995825855623329.\n",
      "The error is 1.699061666431302e-07 for weight 1.9991859114045374 and prediction 0.9995878032428037.\n",
      "The error is 1.65685060315573e-07 for weight 1.9991960875119807 and prediction 0.9995929557022687.\n",
      "The error is 1.6156882209832673e-07 for weight 1.999206136418081 and prediction 0.9995980437559904.\n",
      "The error is 1.5755484667427958e-07 for weight 1.999216059712855 and prediction 0.9996030682090405.\n",
      "The error is 1.5364059345225135e-07 for weight 1.9992258589664442 and prediction 0.9996080298564275.\n",
      "The error is 1.4982358495870099e-07 for weight 1.9992355357293636 and prediction 0.9996129294832221.\n",
      "The error is 1.461014052698981e-07 for weight 1.9992450915327467 and prediction 0.9996177678646818.\n",
      "The error is 1.4247169848269893e-07 for weight 1.9992545278885874 and prediction 0.9996225457663733.\n",
      "The error is 1.3893216722349971e-07 for weight 1.99926384628998 and prediction 0.9996272639442937.\n",
      "The error is 1.3548057119401332e-07 for weight 1.9992730482113554 and prediction 0.99963192314499.\n",
      "The error is 1.3211472575338482e-07 for weight 1.9992821351087136 and prediction 0.9996365241056777.\n",
      "The error is 1.2883250053542523e-07 for weight 1.9992911084198546 and prediction 0.9996410675543568.\n",
      "The error is 1.2563181810026795e-07 for weight 1.9992999695646063 and prediction 0.9996455542099273.\n",
      "The error is 1.225106526193676e-07 for weight 1.9993087199450488 and prediction 0.9996499847823032.\n",
      "The error is 1.194670285933427e-07 for weight 1.9993173609457358 and prediction 0.9996543599725244.\n",
      "The error is 1.1649901960169559e-07 for weight 1.999325893933914 and prediction 0.9996586804728679.\n",
      "The error is 1.1360474708347813e-07 for weight 1.99933432025974 and prediction 0.999662946966957.\n",
      "The error is 1.1078237914815345e-07 for weight 1.9993426412564932 and prediction 0.99966716012987.\n",
      "The error is 1.080301294162125e-07 for weight 1.999350858240787 and prediction 0.9996713206282466.\n",
      "The error is 1.0534625588851585e-07 for weight 1.9993589725127772 and prediction 0.9996754291203935.\n",
      "The error is 1.0272905984380511e-07 for weight 1.9993669853563674 and prediction 0.9996794862563886.\n",
      "The error is 1.0017688476333781e-07 for weight 1.9993748980394128 and prediction 0.9996834926781837.\n",
      "The error is 9.768811528248476e-08 for weight 1.9993827118139202 and prediction 0.9996874490197064.\n",
      "The error is 9.526117616840897e-08 for weight 1.9993904279162462 and prediction 0.9996913559069601.\n",
      "The error is 9.289453132298267e-08 for weight 1.9993980475672932 and prediction 0.9996952139581231.\n",
      "The error is 9.058668281040142e-08 for weight 1.999405571972702 and prediction 0.9996990237836466.\n",
      "The error is 8.833616990935773e-08 for weight 1.9994130023230432 and prediction 0.999702785986351.\n",
      "The error is 8.61415681881705e-08 for weight 1.9994203397940051 and prediction 0.9997065011615216.\n",
      "The error is 8.400148860350369e-08 for weight 1.99942758554658 and prediction 0.9997101698970026.\n",
      "The error is 8.191457662104216e-08 for weight 1.9994347407272477 and prediction 0.99971379277329.\n",
      "The error is 7.987951135811629e-08 for weight 1.9994418064681572 and prediction 0.9997173703636238.\n",
      "The error is 7.789500474779372e-08 for weight 1.9994487838873052 and prediction 0.9997209032340786.\n",
      "The error is 7.595980072359684e-08 for weight 1.999455674088714 and prediction 0.9997243919436526.\n",
      "The error is 7.407267442434958e-08 for weight 1.999462478162605 and prediction 0.999727837044357.\n",
      "The error is 7.223243141914651e-08 for weight 1.9994691971855723 and prediction 0.9997312390813025.\n",
      "The error is 7.04379069511036e-08 for weight 1.9994758322207526 and prediction 0.9997345985927861.\n",
      "The error is 6.868796520027548e-08 for weight 1.9994823843179932 and prediction 0.9997379161103763.\n",
      "The error is 6.698149856484407e-08 for weight 1.9994888545140184 and prediction 0.9997411921589966.\n",
      "The error is 6.531742695985031e-08 for weight 1.999495243832593 and prediction 0.9997444272570092.\n",
      "The error is 6.369469713384385e-08 for weight 1.9995015532846856 and prediction 0.9997476219162965.\n",
      "The error is 6.211228200192976e-08 for weight 1.999507783868627 and prediction 0.9997507766423428.\n",
      "The error is 6.056917999594022e-08 for weight 1.9995139365702692 and prediction 0.9997538919343135.\n",
      "The error is 5.906441443041406e-08 for weight 1.999520012363141 and prediction 0.9997569682851346.\n",
      "The error is 5.7597032884391137e-08 for weight 1.9995260122086016 and prediction 0.9997600061815705.\n",
      "The error is 5.616610659868336e-08 for weight 1.999531937055994 and prediction 0.9997630061043008.\n",
      "The error is 5.477072988787232e-08 for weight 1.9995377878427942 and prediction 0.999765968527997.\n",
      "The error is 5.341001956720189e-08 for weight 1.9995435654947593 and prediction 0.9997688939213971.\n",
      "The error is 5.208311439357795e-08 for weight 1.999549270926075 and prediction 0.9997717827473797.\n",
      "The error is 5.0789174520338105e-08 for weight 1.999554905039499 and prediction 0.9997746354630375.\n",
      "The error is 4.9527380965857104e-08 for weight 1.9995604687265052 and prediction 0.9997774525197495.\n",
      "The error is 4.8296935094996957e-08 for weight 1.9995659628674238 and prediction 0.9997802343632526.\n",
      "The error is 4.709705811373605e-08 for weight 1.999571388331581 and prediction 0.9997829814337119.\n",
      "The error is 4.592699057620805e-08 for weight 1.9995767459774363 and prediction 0.9997856941657906.\n",
      "The error is 4.4785991904090955e-08 for weight 1.9995820366527184 and prediction 0.9997883729887181.\n",
      "The error is 4.3673339917712674e-08 for weight 1.9995872611945593 and prediction 0.9997910183263592.\n",
      "The error is 4.258833037915033e-08 for weight 1.9995924204296274 and prediction 0.9997936305972797.\n",
      "The error is 4.153027654628426e-08 for weight 1.999597515174257 and prediction 0.9997962102148137.\n",
      "The error is 4.049850873832299e-08 for weight 1.9996025462345788 and prediction 0.9997987575871286.\n",
      "The error is 3.949237391187624e-08 for weight 1.9996075144066465 and prediction 0.9998012731172894.\n",
      "The error is 3.851123524750742e-08 for weight 1.9996124204765635 and prediction 0.9998037572033233.\n",
      "The error is 3.755447174681371e-08 for weight 1.9996172652206066 and prediction 0.9998062102382818.\n",
      "The error is 3.6621477839335215e-08 for weight 1.999622049405349 and prediction 0.9998086326103033.\n",
      "The error is 3.5711662999273673e-08 for weight 1.999626773787782 and prediction 0.9998110247026745.\n",
      "The error is 3.4824451371653595e-08 for weight 1.9996314391154346 and prediction 0.999813386893891.\n",
      "The error is 3.3959281407904426e-08 for weight 1.9996360461264917 and prediction 0.9998157195577173.\n",
      "The error is 3.3115605510422755e-08 for weight 1.9996405955499106 and prediction 0.9998180230632459.\n",
      "The error is 3.22928896860192e-08 for weight 1.9996450881055368 and prediction 0.9998202977749553.\n",
      "The error is 3.149061320787133e-08 for weight 1.9996495245042176 and prediction 0.9998225440527684.\n",
      "The error is 3.0708268285975626e-08 for weight 1.999653905447915 and prediction 0.9998247622521088.\n",
      "The error is 2.994535974573055e-08 for weight 1.9996582316298162 and prediction 0.9998269527239575.\n",
      "The error is 2.9201404714530956e-08 for weight 1.9996625037344435 and prediction 0.9998291158149081.\n",
      "The error is 2.8475932316138872e-08 for weight 1.999666722437763 and prediction 0.9998312518672218.\n",
      "The error is 2.7768483372677833e-08 for weight 1.999670888407291 and prediction 0.9998333612188814.\n",
      "The error is 2.7078610113879648e-08 for weight 1.9996750023021999 and prediction 0.9998354442036455.\n",
      "The error is 2.640587589384852e-08 for weight 1.9996790647734224 and prediction 0.9998375011510999.\n",
      "The error is 2.574985491460582e-08 for weight 1.9996830764637545 and prediction 0.9998395323867112.\n",
      "The error is 2.5110131956582077e-08 for weight 1.9996870380079577 and prediction 0.9998415382318773.\n",
      "The error is 2.4486302115777054e-08 for weight 1.9996909500328581 and prediction 0.9998435190039788.\n",
      "The error is 2.3877970547598083e-08 for weight 1.9996948131574475 and prediction 0.9998454750164291.\n",
      "The error is 2.32847522167973e-08 for weight 1.9996986279929794 and prediction 0.9998474065787237.\n",
      "The error is 2.2706271653910403e-08 for weight 1.999702395143067 and prediction 0.9998493139964897.\n",
      "The error is 2.2142162717524263e-08 for weight 1.9997061152037787 and prediction 0.9998511975715335.\n",
      "The error is 2.1592068362505466e-08 for weight 1.9997097887637314 and prediction 0.9998530576018894.\n",
      "The error is 2.1055640414137358e-08 for weight 1.9997134164041848 and prediction 0.9998548943818657.\n",
      "The error is 2.0532539347597838e-08 for weight 1.9997169986991326 and prediction 0.9998567082020924.\n",
      "The error is 2.0022434073166817e-08 for weight 1.9997205362153934 and prediction 0.9998584993495663.\n",
      "The error is 1.9525001726666232e-08 for weight 1.999724029512701 and prediction 0.9998602681076967.\n",
      "The error is 1.9039927465010944e-08 for weight 1.9997274791437922 and prediction 0.9998620147563505.\n",
      "The error is 1.8566904267053966e-08 for weight 1.9997308856544949 and prediction 0.9998637395718961.\n",
      "The error is 1.8105632739164865e-08 for weight 1.9997342495838137 and prediction 0.9998654428272474.\n",
      "The error is 1.7655820925792756e-08 for weight 1.999737571464016 and prediction 0.9998671247919069.\n",
      "The error is 1.7217184124678515e-08 for weight 1.9997408518207158 and prediction 0.999868785732008.\n",
      "The error is 1.678944470657708e-08 for weight 1.999744091172957 and prediction 0.9998704259103579.\n",
      "The error is 1.6372331939640243e-08 for weight 1.9997472900332949 and prediction 0.9998720455864785.\n",
      "The error is 1.5965581818026027e-08 for weight 1.9997504489078788 and prediction 0.9998736450166474.\n",
      "The error is 1.5568936894722667e-08 for weight 1.9997535682965304 and prediction 0.9998752244539394.\n",
      "The error is 1.5182146118733115e-08 for weight 1.9997566486928238 and prediction 0.9998767841482652.\n",
      "The error is 1.4804964676095165e-08 for weight 1.9997596905841635 and prediction 0.9998783243464119.\n",
      "The error is 1.4437153834923092e-08 for weight 1.9997626944518614 and prediction 0.9998798452920817.\n",
      "The error is 1.4078480794341661e-08 for weight 1.999765660771213 and prediction 0.9998813472259307.\n",
      "The error is 1.3728718537110487e-08 for weight 1.999768590011573 and prediction 0.9998828303856065.\n",
      "The error is 1.3387645685955112e-08 for weight 1.9997714826364283 and prediction 0.9998842950057865.\n",
      "The error is 1.3055046363435151e-08 for weight 1.999774339103473 and prediction 0.9998857413182142.\n",
      "The error is 1.2730710055340114e-08 for weight 1.9997771598646796 and prediction 0.9998871695517365.\n",
      "The error is 1.2414431477399975e-08 for weight 1.9997799453663712 and prediction 0.9998885799323398.\n",
      "The error is 1.2106010445380876e-08 for weight 1.9997826960492915 and prediction 0.9998899726831856.\n",
      "The error is 1.1805251748382063e-08 for weight 1.9997854123486754 and prediction 0.9998913480246457.\n",
      "The error is 1.1511965025257303e-08 for weight 1.999788094694317 and prediction 0.9998927061743377.\n",
      "The error is 1.1225964644156361e-08 for weight 1.999790743510638 and prediction 0.9998940473471585.\n",
      "The error is 1.0947069585037396e-08 for weight 1.999793359216755 and prediction 0.999895371755319.\n",
      "The error is 1.0675103325032975e-08 for weight 1.9997959422265454 and prediction 0.9998966796083775.\n",
      "The error is 1.0409893726807917e-08 for weight 1.9997984929487136 and prediction 0.9998979711132727.\n",
      "The error is 1.0151272929535889e-08 for weight 1.9998010117868548 and prediction 0.9998992464743568.\n",
      "The error is 9.899077242683343e-09 for weight 1.999803499139519 and prediction 0.9999005058934274.\n",
      "The error is 9.653147042438701e-09 for weight 1.9998059554002752 and prediction 0.9999017495697595.\n",
      "The error is 9.413326670592342e-09 for weight 1.9998083809577718 and prediction 0.9999029777001376.\n",
      "The error is 9.1794643361129e-09 for weight 1.9998107761957997 and prediction 0.9999041904788859.\n",
      "The error is 8.951412019006294e-09 for weight 1.9998131414933522 and prediction 0.9999053880978999.\n",
      "The error is 8.729025376660662e-09 for weight 1.9998154772246852 and prediction 0.9999065707466761.\n",
      "The error is 8.512163652466931e-09 for weight 1.9998177837593767 and prediction 0.9999077386123426.\n",
      "The error is 8.300689586723173e-09 for weight 1.9998200614623844 and prediction 0.9999088918796883.\n",
      "The error is 8.094469329806016e-09 for weight 1.9998223106941047 and prediction 0.9999100307311922.\n",
      "The error is 7.893372357392414e-09 for weight 1.9998245318104284 and prediction 0.9999111553470523.\n",
      "The error is 7.69727138788309e-09 for weight 1.9998267251627981 and prediction 0.9999122659052142.\n",
      "The error is 7.506042301833156e-09 for weight 1.9998288910982631 and prediction 0.9999133625813991.\n",
      "The error is 7.31956406340055e-09 for weight 1.9998310299595348 and prediction 0.9999144455491316.\n",
      "The error is 7.137718643705601e-09 for weight 1.9998331420850406 and prediction 0.9999155149797674.\n",
      "The error is 6.960390946150114e-09 for weight 1.9998352278089775 and prediction 0.9999165710425203.\n",
      "The error is 6.787468733589701e-09 for weight 1.9998372874613652 and prediction 0.9999176139044887.\n",
      "The error is 6.6188425572418396e-09 for weight 1.9998393213680983 and prediction 0.9999186437306826.\n",
      "The error is 6.454405687452335e-09 for weight 1.999841329850997 and prediction 0.9999196606840491.\n",
      "The error is 6.294054046163279e-09 for weight 1.9998433132278595 and prediction 0.9999206649254985.\n",
      "The error is 6.137686140954997e-09 for weight 1.9998452718125113 and prediction 0.9999216566139297.\n",
      "The error is 5.98520300088764e-09 for weight 1.9998472059148549 and prediction 0.9999226359062556.\n",
      "The error is 5.836508113833913e-09 for weight 1.9998491158409193 and prediction 0.9999236029574274.\n",
      "The error is 5.6915073653733145e-09 for weight 1.9998510018929079 and prediction 0.9999245579204596.\n",
      "The error is 5.550108979259031e-09 for weight 1.9998528643692466 and prediction 0.9999255009464539.\n",
      "The error is 5.412223459301276e-09 for weight 1.999854703564631 and prediction 0.9999264321846233.\n",
      "The error is 5.277763532733051e-09 for weight 1.999856519770073 and prediction 0.9999273517823155.\n",
      "The error is 5.146644094971891e-09 for weight 1.999858313272947 and prediction 0.9999282598850365.\n",
      "The error is 5.0187821557441185e-09 for weight 1.999860084357035 and prediction 0.9999291566364735.\n",
      "The error is 4.894096786569923e-09 for weight 1.9998618333025722 and prediction 0.9999300421785176.\n",
      "The error is 4.772509069528577e-09 for weight 1.99986356038629 and prediction 0.9999309166512861.\n",
      "The error is 4.653942047334559e-09 for weight 1.9998652658814613 and prediction 0.999931780193145.\n",
      "The error is 4.5383206746003914e-09 for weight 1.9998669500579431 and prediction 0.9999326329407306.\n",
      "The error is 4.425571770334326e-09 for weight 1.9998686131822188 and prediction 0.9999334750289716.\n",
      "The error is 4.31562397167037e-09 for weight 1.999870255517441 and prediction 0.9999343065911094.\n",
      "The error is 4.2084076886243645e-09 for weight 1.999871877323473 and prediction 0.9999351277587205.\n",
      "The error is 4.103855060114726e-09 for weight 1.9998734788569295 and prediction 0.9999359386617365.\n",
      "The error is 4.00189991096939e-09 for weight 1.9998750603712179 and prediction 0.9999367394284647.\n",
      "The error is 3.90247771005399e-09 for weight 1.9998766221165776 and prediction 0.9999375301856089.\n",
      "The error is 3.805525529449459e-09 for weight 1.9998781643401204 and prediction 0.9999383110582888.\n",
      "The error is 3.7109820045716198e-09 for weight 1.9998796872858688 and prediction 0.9999390821700602.\n",
      "The error is 3.6187872954012204e-09 for weight 1.9998811911947956 and prediction 0.9999398436429344.\n",
      "The error is 3.528883048525985e-09 for weight 1.9998826763048607 and prediction 0.9999405955973978.\n",
      "The error is 3.4412123602837946e-09 for weight 1.9998841428510499 and prediction 0.9999413381524304.\n",
      "The error is 3.355719740712978e-09 for weight 1.9998855910654116 and prediction 0.9999420714255249.\n",
      "The error is 3.2723510784109915e-09 for weight 1.999887021177094 and prediction 0.9999427955327058.\n",
      "The error is 3.1910536063035824e-09 for weight 1.9998884334123803 and prediction 0.999943510588547.\n",
      "The error is 3.1117758682750743e-09 for weight 1.9998898279947255 and prediction 0.9999442167061902.\n",
      "The error is 3.034467686552355e-09 for weight 1.9998912051447915 and prediction 0.9999449139973627.\n",
      "The error is 2.9590801299617016e-09 for weight 1.9998925650804815 and prediction 0.9999456025723957.\n",
      "The error is 2.8855654829855e-09 for weight 1.9998939080169755 and prediction 0.9999462825402408.\n",
      "The error is 2.813877215516107e-09 for weight 1.9998952341667633 and prediction 0.9999469540084878.\n",
      "The error is 2.7439699534454545e-09 for weight 1.9998965437396787 and prediction 0.9999476170833816.\n",
      "The error is 2.6757994499154055e-09 for weight 1.9998978369429328 and prediction 0.9999482718698394.\n",
      "The error is 2.6093225573304355e-09 for weight 1.9998991139811462 and prediction 0.9999489184714664.\n",
      "The error is 2.5444972000438174e-09 for weight 1.9999003750563817 and prediction 0.9999495569905731.\n",
      "The error is 2.4812823477353444e-09 for weight 1.999901620368177 and prediction 0.9999501875281909.\n",
      "The error is 2.4196379894082483e-09 for weight 1.9999028501135747 and prediction 0.9999508101840885.\n",
      "The error is 2.3595251081114486e-09 for weight 1.999904064487155 and prediction 0.9999514250567874.\n",
      "The error is 2.3009056562046747e-09 for weight 1.9999052636810657 and prediction 0.9999520322435775.\n",
      "The error is 2.2437425313057102e-09 for weight 1.9999064478850523 and prediction 0.9999526318405328.\n",
      "The error is 2.1879995527950124e-09 for weight 1.9999076172864891 and prediction 0.9999532239425262.\n",
      "The error is 2.133641438907569e-09 for weight 1.999908772070408 and prediction 0.9999538086432446.\n",
      "The error is 2.0806337844075566e-09 for weight 1.9999099124195279 and prediction 0.999954386035204.\n",
      "The error is 2.0289430388306823e-09 for weight 1.9999110385142838 and prediction 0.9999549562097639.\n",
      "The error is 1.9785364852103496e-09 for weight 1.9999121505328552 and prediction 0.9999555192571419.\n",
      "The error is 1.929382219406149e-09 for weight 1.9999132486511946 and prediction 0.9999560752664276.\n",
      "The error is 1.881449129890129e-09 for weight 1.9999143330430547 and prediction 0.9999566243255973.\n",
      "The error is 1.8347068780687077e-09 for weight 1.9999154038800164 and prediction 0.9999571665215273.\n",
      "The error is 1.789125879068684e-09 for weight 1.9999164613315161 and prediction 0.9999577019400082.\n",
      "The error is 1.744677283014629e-09 for weight 1.9999175055648721 and prediction 0.9999582306657581.\n",
      "The error is 1.7013329567664516e-09 for weight 1.9999185367453112 and prediction 0.9999587527824361.\n",
      "The error is 1.6590654661242721e-09 for weight 1.9999195550359947 and prediction 0.9999592683726556.\n",
      "The error is 1.6178480584525918e-09 for weight 1.9999205605980448 and prediction 0.9999597775179974.\n",
      "The error is 1.5776546457501897e-09 for weight 1.9999215535905692 and prediction 0.9999602802990224.\n",
      "The error is 1.538459788145269e-09 for weight 1.9999225341706872 and prediction 0.9999607767952846.\n",
      "The error is 1.5002386777793421e-09 for weight 1.9999235024935536 and prediction 0.9999612670853436.\n",
      "The error is 1.462967123131128e-09 for weight 1.9999244587123841 and prediction 0.9999617512467768.\n",
      "The error is 1.426621533666573e-09 for weight 1.9999254029784792 and prediction 0.9999622293561921.\n",
      "The error is 1.3911789049434778e-09 for weight 1.9999263354412482 and prediction 0.9999627014892396.\n",
      "The error is 1.3566168040241973e-09 for weight 1.9999272562482326 and prediction 0.9999631677206241.\n",
      "The error is 1.322913355297707e-09 for weight 1.9999281655451298 and prediction 0.9999636281241163.\n",
      "The error is 1.2900472266263316e-09 for weight 1.9999290634758156 and prediction 0.9999640827725649.\n",
      "The error is 1.2579976158423933e-09 for weight 1.999929950182368 and prediction 0.9999645317379078.\n",
      "The error is 1.226744237570309e-09 for weight 1.9999308258050883 and prediction 0.999964975091184.\n",
      "The error is 1.1962673104205718e-09 for weight 1.9999316904825246 and prediction 0.9999654129025441.\n",
      "The error is 1.166547544429965e-09 for weight 1.999932544351493 and prediction 0.9999658452412623.\n",
      "The error is 1.137566128873782e-09 for weight 1.9999333875470993 and prediction 0.9999662721757465.\n",
      "The error is 1.1093047203632717e-09 for weight 1.9999342202027606 and prediction 0.9999666937735496.\n",
      "The error is 1.0817454312151035e-09 for weight 1.999935042450226 and prediction 0.9999671101013803.\n",
      "The error is 1.0548708181597955e-09 for weight 1.9999358544195982 and prediction 0.999967521225113.\n",
      "The error is 1.0286638712708711e-09 for weight 1.9999366562393532 and prediction 0.9999679272097991.\n",
      "The error is 1.0031080032185457e-09 for weight 1.9999374480363614 and prediction 0.9999683281196766.\n",
      "The error is 9.781870387605467e-10 for weight 1.999938229935907 and prediction 0.9999687240181807.\n",
      "The error is 9.538852045141107e-10 for weight 1.999939002061708 and prediction 0.9999691149679535.\n",
      "The error is 9.301871189671721e-10 for weight 1.9999397645359367 and prediction 0.999969501030854.\n",
      "The error is 9.070777827304986e-10 for weight 1.9999405174792375 and prediction 0.9999698822679683.\n",
      "The error is 8.845425690649623e-10 for weight 1.999941261010747 and prediction 0.9999702587396188.\n",
      "The error is 8.625672146164665e-10 for weight 1.9999419952481126 and prediction 0.9999706305053735.\n",
      "The error is 8.411378103809145e-10 for weight 1.999942720307511 and prediction 0.9999709976240563.\n",
      "The error is 8.202407929063305e-10 for weight 1.9999434363036672 and prediction 0.9999713601537555.\n",
      "The error is 7.998629357067003e-10 for weight 1.9999441433498715 and prediction 0.9999717181518336.\n",
      "The error is 7.79991340895179e-10 for weight 1.9999448415579981 and prediction 0.9999720716749357.\n",
      "The error is 7.606134310186662e-10 for weight 1.9999455310385232 and prediction 0.9999724207789991.\n",
      "The error is 7.417169410892261e-10 for weight 1.9999462119005418 and prediction 0.9999727655192616.\n",
      "The error is 7.232899108318012e-10 for weight 1.999946884251785 and prediction 0.9999731059502709.\n",
      "The error is 7.05320677112301e-10 for weight 1.9999475481986375 and prediction 0.9999734421258925.\n",
      "The error is 6.877978665424033e-10 for weight 1.9999482038461545 and prediction 0.9999737740993188.\n",
      "The error is 6.707103882977188e-10 for weight 1.9999488512980774 and prediction 0.9999741019230772.\n",
      "The error is 6.540474270911447e-10 for weight 1.9999494906568513 and prediction 0.9999744256490387.\n",
      "The error is 6.377984363270828e-10 for weight 1.9999501220236406 and prediction 0.9999747453284257.\n",
      "The error is 6.219531314261738e-10 for weight 1.9999507454983452 and prediction 0.9999750610118203.\n",
      "The error is 6.065014833163479e-10 for weight 1.9999513611796158 and prediction 0.9999753727491726.\n",
      "The error is 5.914337120914899e-10 for weight 1.9999519691648706 and prediction 0.9999756805898079.\n",
      "The error is 5.767402808065836e-10 for weight 1.9999525695503098 and prediction 0.9999759845824353.\n",
      "The error is 5.624118894547027e-10 for weight 1.9999531624309308 and prediction 0.9999762847751549.\n",
      "The error is 5.484394690780774e-10 for weight 1.9999537479005443 and prediction 0.9999765812154654.\n",
      "The error is 5.348141760160507e-10 for weight 1.9999543260517874 and prediction 0.9999768739502721.\n",
      "The error is 5.215273863328071e-10 for weight 1.99995489697614 and prediction 0.9999771630258937.\n",
      "The error is 5.08570690328038e-10 for weight 1.9999554607639383 and prediction 0.99997744848807.\n",
      "The error is 4.959358872407571e-10 for weight 1.9999560175043891 and prediction 0.9999777303819691.\n",
      "The error is 4.836149800403495e-10 for weight 1.9999565672855844 and prediction 0.9999780087521946.\n",
      "The error is 4.716001703778022e-10 for weight 1.9999571101945146 and prediction 0.9999782836427922.\n",
      "The error is 4.5988385364366913e-10 for weight 1.9999576463170832 and prediction 0.9999785550972573.\n",
      "The error is 4.484586141548268e-10 for weight 1.9999581757381197 and prediction 0.9999788231585416.\n",
      "The error is 4.373172204572122e-10 for weight 1.9999586985413933 and prediction 0.9999790878690599.\n",
      "The error is 4.264526207602747e-10 for weight 1.999959214809626 and prediction 0.9999793492706966.\n",
      "The error is 4.1585793846275217e-10 for weight 1.9999597246245056 and prediction 0.999979607404813.\n",
      "The error is 4.05526467804683e-10 for weight 1.9999602280666993 and prediction 0.9999798623122528.\n",
      "The error is 3.954516696195533e-10 for weight 1.9999607252158655 and prediction 0.9999801140333496.\n",
      "The error is 3.8562716720206096e-10 for weight 1.9999612161506672 and prediction 0.9999803626079328.\n",
      "The error is 3.76046742266508e-10 for weight 1.999961700948784 and prediction 0.9999806080753336.\n",
      "The error is 3.667043310113579e-10 for weight 1.9999621796869242 and prediction 0.999980850474392.\n",
      "The error is 3.5759402028700714e-10 for weight 1.9999626524408376 and prediction 0.9999810898434621.\n",
      "The error is 3.487100438473159e-10 for weight 1.999963119285327 and prediction 0.9999813262204188.\n",
      "The error is 3.400467786958424e-10 for weight 1.9999635802942606 and prediction 0.9999815596426636.\n",
      "The error is 3.315987415365056e-10 for weight 1.9999640355405823 and prediction 0.9999817901471303.\n",
      "The error is 3.233605853013083e-10 for weight 1.999964485096325 and prediction 0.9999820177702912.\n",
      "The error is 3.153270957618061e-10 for weight 1.999964929032621 and prediction 0.9999822425481625.\n",
      "The error is 3.074931882263278e-10 for weight 1.999965367419713 and prediction 0.9999824645163105.\n",
      "The error is 2.9985390433229115e-10 for weight 1.9999658003269667 and prediction 0.9999826837098565.\n",
      "The error is 2.924044088962985e-10 for weight 1.9999662278228796 and prediction 0.9999829001634833.\n",
      "The error is 2.851399868632029e-10 for weight 1.9999666499750937 and prediction 0.9999831139114398.\n",
      "The error is 2.7805604031341316e-10 for weight 1.999967066850405 and prediction 0.9999833249875468.\n",
      "The error is 2.711480855610084e-10 for weight 1.999967478514775 and prediction 0.9999835334252025.\n",
      "The error is 2.6441175031053266e-10 for weight 1.9999678850333402 and prediction 0.9999837392573875.\n",
      "The error is 2.5784277088969127e-10 for weight 1.9999682864704236 and prediction 0.9999839425166701.\n",
      "The error is 2.514369895487281e-10 for weight 1.9999686828895433 and prediction 0.9999841432352118.\n",
      "The error is 2.45190351838888e-10 for weight 1.999969074353424 and prediction 0.9999843414447717.\n",
      "The error is 2.3909890403637777e-10 for weight 1.9999694609240062 and prediction 0.999984537176712.\n",
      "The error is 2.331587906390545e-10 for weight 1.9999698426624561 and prediction 0.9999847304620031.\n",
      "The error is 2.2736625193352954e-10 for weight 1.9999702196291755 and prediction 0.9999849213312281.\n",
      "The error is 2.2171762161060943e-10 for weight 1.9999705918838109 and prediction 0.9999851098145878.\n",
      "The error is 2.162093244482311e-10 for weight 1.9999709594852633 and prediction 0.9999852959419054.\n",
      "The error is 2.1083787404360764e-10 for weight 1.9999713224916975 and prediction 0.9999854797426316.\n",
      "The error is 2.055998706098194e-10 for weight 1.9999716809605512 and prediction 0.9999856612458488.\n",
      "The error is 2.004919988253392e-10 for weight 1.9999720349485444 and prediction 0.9999858404802756.\n",
      "The error is 1.955110257290953e-10 for weight 1.9999723845116875 and prediction 0.9999860174742722.\n",
      "The error is 1.9065379868497942e-10 for weight 1.9999727297052914 and prediction 0.9999861922558437.\n",
      "The error is 1.8591724337367239e-10 for weight 1.9999730705839753 and prediction 0.9999863648526457.\n",
      "The error is 1.812983618574118e-10 for weight 1.9999734072016757 and prediction 0.9999865352919877.\n",
      "The error is 1.7679423067905722e-10 for weight 1.9999737396116548 and prediction 0.9999867036008379.\n",
      "The error is 1.724019990100413e-10 for weight 1.9999740678665092 and prediction 0.9999868698058274.\n",
      "The error is 1.6811888684591196e-10 for weight 1.9999743920181778 and prediction 0.9999870339332546.\n",
      "The error is 1.6394218325104707e-10 for weight 1.9999747121179505 and prediction 0.9999871960090889.\n",
      "The error is 1.5986924463702688e-10 for weight 1.999975028216476 and prediction 0.9999873560589753.\n",
      "The error is 1.5589749309109558e-10 for weight 1.9999753403637701 and prediction 0.999987514108238.\n",
      "The error is 1.5202441474755857e-10 for weight 1.999975648609223 and prediction 0.9999876701818851.\n",
      "The error is 1.4824755819286282e-10 for weight 1.9999759530016077 and prediction 0.9999878243046115.\n",
      "The error is 1.4456453291997668e-10 for weight 1.9999762535890875 and prediction 0.9999879765008038.\n",
      "The error is 1.4097300780653125e-10 for weight 1.999976550419224 and prediction 0.9999881267945437.\n",
      "The error is 1.3747070964308925e-10 for weight 1.9999768435389838 and prediction 0.999988275209612.\n",
      "The error is 1.34055421699269e-10 for weight 1.9999771329947464 and prediction 0.9999884217694919.\n",
      "The error is 1.3072498231655471e-10 for weight 1.999977418832312 and prediction 0.9999885664973732.\n",
      "The error is 1.2747728353772323e-10 for weight 1.9999777010969082 and prediction 0.999988709416156.\n",
      "The error is 1.2431026977436873e-10 for weight 1.999977979833197 and prediction 0.9999888505484541.\n",
      "The error is 1.2122193650889777e-10 for weight 1.999978255085282 and prediction 0.9999889899165985.\n",
      "The error is 1.1821032902281935e-10 for weight 1.999978526896716 and prediction 0.999989127542641.\n",
      "The error is 1.1527354116059249e-10 for weight 1.999978795310507 and prediction 0.999989263448358.\n",
      "The error is 1.1240971412320799e-10 for weight 1.9999790603691256 and prediction 0.9999893976552535.\n",
      "The error is 1.0961703528851167e-10 for weight 1.9999793221145117 and prediction 0.9999895301845628.\n",
      "The error is 1.0689373706700094e-10 for weight 1.9999795805880802 and prediction 0.9999896610572558.\n",
      "The error is 1.042380957873094e-10 for weight 1.9999798358307292 and prediction 0.9999897902940401.\n",
      "The error is 1.0164843059489755e-10 for weight 1.999980087882845 and prediction 0.9999899179153646.\n",
      "The error is 9.912310239769242e-11 for weight 1.9999803367843094 and prediction 0.9999900439414225.\n",
      "The error is 9.666051282356399e-11 for weight 1.9999805825745056 and prediction 0.9999901683921547.\n",
      "The error is 9.425910320748379e-11 for weight 1.9999808252923243 and prediction 0.9999902912872528.\n",
      "The error is 9.191735361166727e-11 for weight 1.9999810649761702 and prediction 0.9999904126461622.\n",
      "The error is 8.96337818584818e-11 for weight 1.999981301663968 and prediction 0.9999905324880851.\n",
      "The error is 8.740694259105792e-11 for weight 1.9999815353931685 and prediction 0.999990650831984.\n",
      "The error is 8.523542636070258e-11 for weight 1.999981766200754 and prediction 0.9999907676965842.\n",
      "The error is 8.311785873619352e-11 for weight 1.9999819941232446 and prediction 0.999990883100377.\n",
      "The error is 8.105289943246657e-11 for weight 1.999982219196704 and prediction 0.9999909970616223.\n",
      "The error is 7.903924146233928e-11 for weight 1.9999824414567453 and prediction 0.999991109598352.\n",
      "The error is 7.70756103067232e-11 for weight 1.999982660938536 and prediction 0.9999912207283727.\n",
      "The error is 7.516076311266024e-11 for weight 1.9999828776768043 and prediction 0.999991330469268.\n",
      "The error is 7.329348790474542e-11 for weight 1.9999830917058443 and prediction 0.9999914388384021.\n",
      "The error is 7.147260281376716e-11 for weight 1.9999833030595213 and prediction 0.9999915458529222.\n",
      "The error is 6.969695533712602e-11 for weight 1.9999835117712772 and prediction 0.9999916515297607.\n",
      "The error is 6.796542160383882e-11 for weight 1.9999837178741362 and prediction 0.9999917558856386.\n",
      "The error is 6.627690566113961e-11 for weight 1.9999839214007096 and prediction 0.9999918589370681.\n",
      "The error is 6.46303387855182e-11 for weight 1.9999841223832007 and prediction 0.9999919607003548.\n",
      "The error is 6.302467880660193e-11 for weight 1.9999843208534107 and prediction 0.9999920611916003.\n",
      "The error is 6.145890944208699e-11 for weight 1.999984516842743 and prediction 0.9999921604267054.\n",
      "The error is 5.993203966100043e-11 for weight 1.9999847103822088 and prediction 0.9999922584213715.\n",
      "The error is 5.844310305009955e-11 for weight 1.9999849015024311 and prediction 0.9999923551911044.\n",
      "The error is 5.6991157209264375e-11 for weight 1.9999850902336507 and prediction 0.9999924507512156.\n",
      "The error is 5.5575283147863996e-11 for weight 1.99998527660573 and prediction 0.9999925451168253.\n",
      "The error is 5.419458470746574e-11 for weight 1.9999854606481584 and prediction 0.999992638302865.\n",
      "The error is 5.2848187993397513e-11 for weight 1.9999856423900564 and prediction 0.9999927303240792.\n",
      "The error is 5.1535240823255346e-11 for weight 1.9999858218601807 and prediction 0.9999928211950282.\n",
      "The error is 5.025491218389519e-11 for weight 1.9999859990869284 and prediction 0.9999929109300904.\n",
      "The error is 4.9006391709462554e-11 for weight 1.9999861740983418 and prediction 0.9999929995434642.\n",
      "The error is 4.778888916566084e-11 for weight 1.9999863469221124 and prediction 0.9999930870491709.\n",
      "The error is 4.660163395113356e-11 for weight 1.999986517585586 and prediction 0.9999931734610562.\n",
      "The error is 4.544387460779106e-11 for weight 1.9999866861157662 and prediction 0.999993258792793.\n",
      "The error is 4.431487834746793e-11 for weight 1.9999868525393192 and prediction 0.9999933430578831.\n",
      "The error is 4.3213930588413545e-11 for weight 1.9999870168825777 and prediction 0.9999934262696596.\n",
      "The error is 4.214033450005135e-11 for weight 1.9999871791715456 and prediction 0.9999935084412889.\n",
      "The error is 4.109341056454881e-11 for weight 1.9999873394319012 and prediction 0.9999935895857728.\n",
      "The error is 4.00724961460115e-11 for weight 1.9999874976890024 and prediction 0.9999936697159506.\n",
      "The error is 3.907694507009223e-11 for weight 1.9999876539678898 and prediction 0.9999937488445012.\n",
      "The error is 3.8106127216521135e-11 for weight 1.9999878082932911 and prediction 0.9999938269839449.\n",
      "The error is 3.71594281186718e-11 for weight 1.999987960689625 and prediction 0.9999939041466456.\n",
      "The error is 3.6236248576866493e-11 for weight 1.9999881111810045 and prediction 0.9999939803448125.\n",
      "The error is 3.533600427692843e-11 for weight 1.999988259791242 and prediction 0.9999940555905023.\n",
      "The error is 3.44581254206409e-11 for weight 1.9999884065438513 and prediction 0.999994129895621.\n",
      "The error is 3.360205636767235e-11 for weight 1.9999885514620532 and prediction 0.9999942032719257.\n",
      "The error is 3.2767255279645e-11 for weight 1.9999886945687775 and prediction 0.9999942757310266.\n",
      "The error is 3.1953193781777695e-11 for weight 1.9999888358866678 and prediction 0.9999943472843887.\n",
      "The error is 3.115935662377715e-11 for weight 1.9999889754380844 and prediction 0.9999944179433339.\n",
      "The error is 3.038524135770108e-11 for weight 1.9999891132451084 and prediction 0.9999944877190422.\n",
      "The error is 2.9630358017342984e-11 for weight 1.9999892493295446 and prediction 0.9999945566225542.\n",
      "The error is 2.8894228810245183e-11 for weight 1.9999893837129252 and prediction 0.9999946246647723.\n",
      "The error is 2.8176387813491117e-11 for weight 1.9999895164165136 and prediction 0.9999946918564626.\n",
      "The error is 2.7476380679244358e-11 for weight 1.9999896474613073 and prediction 0.9999947582082568.\n",
      "The error is 2.67937643462128e-11 for weight 1.9999897768680408 and prediction 0.9999948237306536.\n",
      "The error is 2.6128106763789888e-11 for weight 1.9999899046571903 and prediction 0.9999948884340204.\n",
      "The error is 2.5478986611405003e-11 for weight 1.9999900308489753 and prediction 0.9999949523285951.\n",
      "The error is 2.4845993038275967e-11 for weight 1.999990155463363 and prediction 0.9999950154244877.\n",
      "The error is 2.4228725399277782e-11 for weight 1.999990278520071 and prediction 0.9999950777316815.\n",
      "The error is 2.3626793002774387e-11 for weight 1.99999040003857 and prediction 0.9999951392600355.\n",
      "The error is 2.30398148644581e-11 for weight 1.9999905200380879 and prediction 0.999995200019285.\n",
      "The error is 2.2467419463906062e-11 for weight 1.9999906385376118 and prediction 0.9999952600190439.\n",
      "The error is 2.1909244511274855e-11 for weight 1.9999907555558918 and prediction 0.9999953192688059.\n",
      "The error is 2.136493671747319e-11 for weight 1.9999908711114431 and prediction 0.9999953777779459.\n",
      "The error is 2.0834151570911134e-11 for weight 1.9999909852225501 and prediction 0.9999954355557216.\n",
      "The error is 2.031655311760863e-11 for weight 1.9999910979072684 and prediction 0.9999954926112751.\n",
      "The error is 1.9811813750648292e-11 for weight 1.9999912091834275 and prediction 0.9999955489536342.\n",
      "The error is 1.9319614002646426e-11 for weight 1.9999913190686347 and prediction 0.9999956045917138.\n",
      "The error is 1.883964234226818e-11 for weight 1.9999914275802768 and prediction 0.9999956595343173.\n",
      "The error is 1.8371594977613317e-11 for weight 1.9999915347355233 and prediction 0.9999957137901384.\n",
      "The error is 1.791517566525242e-11 for weight 1.9999916405513292 and prediction 0.9999957673677616.\n",
      "The error is 1.747009552008563e-11 for weight 1.9999917450444376 and prediction 0.9999958202756646.\n",
      "The error is 1.7036072834497045e-11 for weight 1.999991848231382 and prediction 0.9999958725222188.\n",
      "The error is 1.6612832899958446e-11 for weight 1.9999919501284897 and prediction 0.999995924115691.\n",
      "The error is 1.6200107832935247e-11 for weight 1.9999920507518836 and prediction 0.9999959750642449.\n",
      "The error is 1.5797636404115206e-11 for weight 1.999992150117485 and prediction 0.9999960253759418.\n",
      "The error is 1.5405163874428123e-11 for weight 1.9999922482410166 and prediction 0.9999960750587426.\n",
      "The error is 1.502244183408931e-11 for weight 1.999992345138004 and prediction 0.9999961241205083.\n",
      "The error is 1.4649228044656796e-11 for weight 1.9999924408237788 and prediction 0.999996172569002.\n",
      "The error is 1.428528628576854e-11 for weight 1.9999925353134815 and prediction 0.9999962204118894.\n",
      "The error is 1.393038620474115e-11 for weight 1.999992628622063 and prediction 0.9999962676567408.\n",
      "The error is 1.3584303172293203e-11 for weight 1.9999927207642874 and prediction 0.9999963143110315.\n",
      "The error is 1.3246818140053486e-11 for weight 1.9999928117547339 and prediction 0.9999963603821437.\n",
      "The error is 1.291771750151743e-11 for weight 1.9999929016077997 and prediction 0.9999964058773669.\n",
      "The error is 1.2596792957161788e-11 for weight 1.9999929903377023 and prediction 0.9999964508038999.\n",
      "The error is 1.2283841382015566e-11 for weight 1.999993077958481 and prediction 0.9999964951688511.\n",
      "The error is 1.1978664697988518e-11 for weight 1.9999931644839999 and prediction 0.9999965389792405.\n",
      "The error is 1.1681069746954782e-11 for weight 1.9999932499279498 and prediction 0.9999965822419999.\n",
      "The error is 1.1390868170719271e-11 for weight 1.9999933343038505 and prediction 0.9999966249639749.\n",
      "The error is 1.1107876289325449e-11 for weight 1.9999934176250524 and prediction 0.9999966671519253.\n",
      "The error is 1.0831914987534148e-11 for weight 1.9999934999047393 and prediction 0.9999967088125262.\n",
      "The error is 1.0562809599571616e-11 for weight 1.99999358115593 and prediction 0.9999967499523696.\n",
      "The error is 1.0300389798885128e-11 for weight 1.9999936613914808 and prediction 0.999996790577965.\n",
      "The error is 1.004448948983667e-11 for weight 1.9999937406240873 and prediction 0.9999968306957404.\n",
      "The error is 9.794946704116973e-12 for weight 1.9999938188662862 and prediction 0.9999968703120437.\n",
      "The error is 9.55160349683363e-12 for weight 1.9999938961304577 and prediction 0.9999969094331431.\n",
      "The error is 9.314305847205046e-12 for weight 1.999993972428827 and prediction 0.9999969480652289.\n"
     ]
    }
   ],
   "source": [
    "input = 0.5\n",
    "goal_prediction = 1\n",
    "weight = 0.25\n",
    "alpha_coefficient = 0.05\n",
    "\n",
    "for i in range(1000):\n",
    "    prediction = input * weight\n",
    "    q_error = (prediction - goal_prediction) ** 2\n",
    "    delta = prediction - goal_prediction\n",
    "    delta_weight = delta * input # derivative\n",
    "    weight = weight - (delta_weight * alpha_coefficient)\n",
    "    print(\"The error is {0} for weight {1} and prediction {2}.\". format(q_error, weight, prediction))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Congratulations, we have done this task. We will learn about multi-input and multi-output gradient descent next time!\n",
    " See you soon :)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}